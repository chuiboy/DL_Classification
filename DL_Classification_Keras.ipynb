{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Deep Learning: Classification with Keras\n",
    "___\n",
    "\n",
    "#### Summary:\n",
    "\n",
    "Deep Learning has grown in popularity recently due to better GPUs and the large amount of data we have access to. Combining the large amount of data with better GPUs, we are able to train deep neural networks in a reasonable amount of time that outperform traditional machine learning models such as Logistic/Softmax Regression, SVM, Naive Bayes, K-Nearest Neighbor, etc.\n",
    "\n",
    "When building a deep neural network, it is common to use APIs like Tensorflow and Keras. Tensorflow is an API that runs on top of Python and contains many useful low-level functions. Keras is an API that runs on top of TensorFlow and has many high-level functions but is usually more restrictive. When building a neural network, Keras will usually contain all the tools we need. Note that we will not get into hyperparameter tuning here and we will not use a validation set (The test set is used as the validaiton set here).\n",
    "___\n",
    "#### This notebook will include:\n",
    "1. Softmax Regression\n",
    "2. 3-Layer Standard Neural Network (Multilayer Perceptron)\n",
    "3. 8-Layer Standard Neural Network\n",
    "4. 3-Layer Convolutional Network\n",
    "5. 8-Layer Convolutional Network\n",
    "6. Inception Network\n",
    "7. Residual Network\n",
    "___\n",
    "#### Reference: \n",
    "\n",
    "Much of what is in this notebook was learned from the Deep Learning Specialization Coursera course by Andrew Ng, the Udemy A-Z: Deep Learning course, and the Tensorflow tutorial at https://www.tensorflow.org/get_started/mnist/pros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Datasets/MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting Datasets/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting Datasets/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting Datasets/MNIST/t10k-labels-idx1-ubyte.gz\n",
      "X_train: (55000, 784)\n",
      "X_test: (10000, 784)\n",
      "y_train: (55000, 10)\n",
      "y_test: (10000, 10)\n",
      "X_train_image: (55000, 28, 28, 1)\n",
      "X_test_image: (10000, 28, 28, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x24605240>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xee13a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The dataset that will be used for this notebook is the MNIST dataset which consists of hand-drawn digits\n",
    "ranging from 0 to 9. The dataset is separated into 55000 training examples, 5000 validation examples\n",
    "and 10000 test examples. Each example consists of 784 input features corresponding to the 784 pixel \n",
    "values of the 28x28 sized image. The dataset has already been preprocessed (divided by 255).\n",
    "\"\"\"\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, MaxPooling2D, ZeroPadding2D, Conv2D\n",
    "from keras.layers import Concatenate, Input, BatchNormalization, Activation, Add, Flatten\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Import the dataset (one-hot encoded)\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "dataset = input_data.read_data_sets(\"Datasets/MNIST/\", one_hot=True)\n",
    "\n",
    "X_train = dataset.train.images\n",
    "y_train = dataset.train.labels\n",
    "X_test = dataset.test.images\n",
    "y_test = dataset.test.labels\n",
    "\n",
    "X_train_image = X_train.reshape( -1, 28, 28, 1)\n",
    "X_test_image = X_test.reshape( -1, 28, 28, 1)\n",
    "\n",
    "# Printing the dataset shape\n",
    "print('X_train:', X_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('y_test:', y_test.shape)\n",
    "print('X_train_image:', X_train_image.shape)\n",
    "print('X_test_image:', X_test_image.shape)\n",
    "\n",
    "# Display an example from the dataset\n",
    "sample = Image.fromarray(255*X_train[np.random.randint(X_train.shape[0]), :].reshape(28,28))\n",
    "plt.imshow(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "55000/55000 [==============================] - 2s 40us/step - loss: 0.6830 - acc: 0.8337\n",
      "Epoch 2/100\n",
      "55000/55000 [==============================] - 2s 37us/step - loss: 0.3703 - acc: 0.8994\n",
      "Epoch 3/100\n",
      "55000/55000 [==============================] - 2s 37us/step - loss: 0.3259 - acc: 0.9103\n",
      "Epoch 4/100\n",
      "55000/55000 [==============================] - 2s 37us/step - loss: 0.3050 - acc: 0.9150\n",
      "Epoch 5/100\n",
      "55000/55000 [==============================] - 2s 36us/step - loss: 0.2923 - acc: 0.9184\n",
      "Epoch 6/100\n",
      "55000/55000 [==============================] - 2s 37us/step - loss: 0.2838 - acc: 0.9208\n",
      "Epoch 7/100\n",
      "55000/55000 [==============================] - 2s 39us/step - loss: 0.2774 - acc: 0.9226\n",
      "Epoch 8/100\n",
      "55000/55000 [==============================] - 2s 38us/step - loss: 0.2722 - acc: 0.9241\n",
      "Epoch 9/100\n",
      "55000/55000 [==============================] - 2s 36us/step - loss: 0.2684 - acc: 0.9253\n",
      "Epoch 10/100\n",
      "55000/55000 [==============================] - 2s 37us/step - loss: 0.2654 - acc: 0.9254\n",
      "Epoch 11/100\n",
      "55000/55000 [==============================] - 2s 37us/step - loss: 0.2625 - acc: 0.9272\n",
      "Epoch 12/100\n",
      "55000/55000 [==============================] - 2s 38us/step - loss: 0.2599 - acc: 0.9276\n",
      "Epoch 13/100\n",
      "55000/55000 [==============================] - 2s 38us/step - loss: 0.2575 - acc: 0.9283\n",
      "Epoch 14/100\n",
      "55000/55000 [==============================] - 2s 38us/step - loss: 0.2556 - acc: 0.9296\n",
      "Epoch 15/100\n",
      "55000/55000 [==============================] - 2s 38us/step - loss: 0.2536 - acc: 0.9295\n",
      "Epoch 16/100\n",
      "55000/55000 [==============================] - 2s 38us/step - loss: 0.2522 - acc: 0.9304\n",
      "Epoch 17/100\n",
      "55000/55000 [==============================] - 2s 38us/step - loss: 0.2510 - acc: 0.9302\n",
      "Epoch 18/100\n",
      "55000/55000 [==============================] - 2s 38us/step - loss: 0.2496 - acc: 0.9307\n",
      "Epoch 19/100\n",
      "55000/55000 [==============================] - 2s 36us/step - loss: 0.2485 - acc: 0.9312\n",
      "Epoch 20/100\n",
      "55000/55000 [==============================] - 2s 39us/step - loss: 0.2473 - acc: 0.9318\n",
      "Epoch 21/100\n",
      "55000/55000 [==============================] - 2s 37us/step - loss: 0.2464 - acc: 0.9321\n",
      "Epoch 22/100\n",
      "55000/55000 [==============================] - 2s 35us/step - loss: 0.2455 - acc: 0.9320\n",
      "Epoch 23/100\n",
      "55000/55000 [==============================] - 2s 34us/step - loss: 0.2441 - acc: 0.9331\n",
      "Epoch 24/100\n",
      "55000/55000 [==============================] - 2s 35us/step - loss: 0.2436 - acc: 0.9328\n",
      "Epoch 25/100\n",
      "55000/55000 [==============================] - 2s 35us/step - loss: 0.2427 - acc: 0.9333\n",
      "Epoch 26/100\n",
      "55000/55000 [==============================] - 2s 35us/step - loss: 0.2423 - acc: 0.9327\n",
      "Epoch 27/100\n",
      "55000/55000 [==============================] - 2s 34us/step - loss: 0.2412 - acc: 0.9333\n",
      "Epoch 28/100\n",
      "55000/55000 [==============================] - 2s 35us/step - loss: 0.2406 - acc: 0.9335\n",
      "Epoch 29/100\n",
      "55000/55000 [==============================] - 2s 35us/step - loss: 0.2400 - acc: 0.9342\n",
      "Epoch 30/100\n",
      "55000/55000 [==============================] - 2s 35us/step - loss: 0.2387 - acc: 0.9343\n",
      "Epoch 31/100\n",
      "55000/55000 [==============================] - 2s 35us/step - loss: 0.2388 - acc: 0.9343\n",
      "Epoch 32/100\n",
      "55000/55000 [==============================] - 2s 35us/step - loss: 0.2384 - acc: 0.9347\n",
      "Epoch 33/100\n",
      "55000/55000 [==============================] - 2s 35us/step - loss: 0.2375 - acc: 0.9346\n",
      "Epoch 34/100\n",
      "55000/55000 [==============================] - 2s 35us/step - loss: 0.2370 - acc: 0.9342\n",
      "Epoch 35/100\n",
      "55000/55000 [==============================] - 2s 34us/step - loss: 0.2366 - acc: 0.9345\n",
      "Epoch 36/100\n",
      "55000/55000 [==============================] - 2s 35us/step - loss: 0.2362 - acc: 0.9345\n",
      "Epoch 37/100\n",
      "55000/55000 [==============================] - 2s 35us/step - loss: 0.2353 - acc: 0.9350\n",
      "Epoch 38/100\n",
      "55000/55000 [==============================] - 2s 35us/step - loss: 0.2352 - acc: 0.9352\n",
      "Epoch 39/100\n",
      "55000/55000 [==============================] - 2s 35us/step - loss: 0.2348 - acc: 0.9351\n",
      "Epoch 40/100\n",
      "55000/55000 [==============================] - 2s 35us/step - loss: 0.2343 - acc: 0.9359\n",
      "Epoch 41/100\n",
      "55000/55000 [==============================] - 2s 35us/step - loss: 0.2336 - acc: 0.9352\n",
      "Epoch 42/100\n",
      "55000/55000 [==============================] - 2s 35us/step - loss: 0.2332 - acc: 0.9355\n",
      "Epoch 43/100\n",
      "55000/55000 [==============================] - 2s 34us/step - loss: 0.2328 - acc: 0.9356\n",
      "Epoch 44/100\n",
      "55000/55000 [==============================] - 2s 36us/step - loss: 0.2328 - acc: 0.9355\n",
      "Epoch 45/100\n",
      "55000/55000 [==============================] - 2s 35us/step - loss: 0.2324 - acc: 0.9360\n",
      "Epoch 46/100\n",
      "55000/55000 [==============================] - 2s 35us/step - loss: 0.2319 - acc: 0.9353\n",
      "Epoch 47/100\n",
      "55000/55000 [==============================] - 2s 35us/step - loss: 0.2319 - acc: 0.9354\n",
      "Epoch 48/100\n",
      "55000/55000 [==============================] - 2s 35us/step - loss: 0.2312 - acc: 0.9362\n",
      "Epoch 49/100\n",
      "55000/55000 [==============================] - 2s 35us/step - loss: 0.2309 - acc: 0.9366\n",
      "Epoch 50/100\n",
      "55000/55000 [==============================] - 2s 35us/step - loss: 0.2307 - acc: 0.9364\n",
      "Epoch 51/100\n",
      "55000/55000 [==============================] - 2s 35us/step - loss: 0.2303 - acc: 0.9365\n",
      "Epoch 52/100\n",
      "55000/55000 [==============================] - 2s 36us/step - loss: 0.2299 - acc: 0.9361\n",
      "Epoch 53/100\n",
      "55000/55000 [==============================] - 2s 35us/step - loss: 0.2296 - acc: 0.9366\n",
      "Epoch 54/100\n",
      "55000/55000 [==============================] - 2s 35us/step - loss: 0.2293 - acc: 0.9364\n",
      "Epoch 55/100\n",
      "55000/55000 [==============================] - 2s 35us/step - loss: 0.2296 - acc: 0.9366\n",
      "Epoch 56/100\n",
      "55000/55000 [==============================] - 2s 35us/step - loss: 0.2290 - acc: 0.9362\n",
      "Epoch 57/100\n",
      "55000/55000 [==============================] - 2s 35us/step - loss: 0.2285 - acc: 0.9365\n",
      "Epoch 58/100\n",
      "55000/55000 [==============================] - 2s 35us/step - loss: 0.2284 - acc: 0.9366\n",
      "Epoch 59/100\n",
      "55000/55000 [==============================] - 2s 34us/step - loss: 0.2283 - acc: 0.9368\n",
      "Epoch 60/100\n",
      "55000/55000 [==============================] - 2s 39us/step - loss: 0.2277 - acc: 0.9364\n",
      "Epoch 61/100\n",
      "55000/55000 [==============================] - 2s 39us/step - loss: 0.2279 - acc: 0.9365\n",
      "Epoch 62/100\n",
      "55000/55000 [==============================] - 2s 39us/step - loss: 0.2277 - acc: 0.9368\n",
      "Epoch 63/100\n",
      "55000/55000 [==============================] - 2s 39us/step - loss: 0.2271 - acc: 0.9368\n",
      "Epoch 64/100\n",
      "55000/55000 [==============================] - 2s 38us/step - loss: 0.2268 - acc: 0.9372\n",
      "Epoch 65/100\n",
      "55000/55000 [==============================] - 2s 39us/step - loss: 0.2270 - acc: 0.9375\n",
      "Epoch 66/100\n",
      "55000/55000 [==============================] - 2s 38us/step - loss: 0.2265 - acc: 0.9369: 0s - loss: 0.2266 - acc: 0.936\n",
      "Epoch 67/100\n",
      "55000/55000 [==============================] - 2s 39us/step - loss: 0.2264 - acc: 0.9371\n",
      "Epoch 68/100\n",
      "55000/55000 [==============================] - 2s 38us/step - loss: 0.2262 - acc: 0.9372\n",
      "Epoch 69/100\n",
      "55000/55000 [==============================] - 2s 39us/step - loss: 0.2263 - acc: 0.9373\n",
      "Epoch 70/100\n",
      "55000/55000 [==============================] - 2s 39us/step - loss: 0.2257 - acc: 0.9375\n",
      "Epoch 71/100\n",
      "55000/55000 [==============================] - 2s 39us/step - loss: 0.2256 - acc: 0.9370\n",
      "Epoch 72/100\n",
      "55000/55000 [==============================] - 2s 43us/step - loss: 0.2253 - acc: 0.9375\n",
      "Epoch 73/100\n",
      "55000/55000 [==============================] - 2s 39us/step - loss: 0.2251 - acc: 0.9373\n",
      "Epoch 74/100\n",
      "55000/55000 [==============================] - 2s 38us/step - loss: 0.2249 - acc: 0.9377\n",
      "Epoch 75/100\n",
      "55000/55000 [==============================] - 2s 38us/step - loss: 0.2249 - acc: 0.9377\n",
      "Epoch 76/100\n",
      "55000/55000 [==============================] - 2s 38us/step - loss: 0.2245 - acc: 0.9374\n",
      "Epoch 77/100\n",
      "55000/55000 [==============================] - 2s 39us/step - loss: 0.2249 - acc: 0.9374\n",
      "Epoch 78/100\n",
      "55000/55000 [==============================] - 2s 40us/step - loss: 0.2244 - acc: 0.9371\n",
      "Epoch 79/100\n",
      "55000/55000 [==============================] - 2s 41us/step - loss: 0.2240 - acc: 0.9377\n",
      "Epoch 80/100\n",
      "55000/55000 [==============================] - 2s 40us/step - loss: 0.2241 - acc: 0.9374\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55000/55000 [==============================] - 2s 41us/step - loss: 0.2239 - acc: 0.9378\n",
      "Epoch 82/100\n",
      "55000/55000 [==============================] - 2s 37us/step - loss: 0.2236 - acc: 0.9379\n",
      "Epoch 83/100\n",
      "55000/55000 [==============================] - 2s 40us/step - loss: 0.2236 - acc: 0.9380\n",
      "Epoch 84/100\n",
      "55000/55000 [==============================] - 2s 37us/step - loss: 0.2232 - acc: 0.9379\n",
      "Epoch 85/100\n",
      "55000/55000 [==============================] - 2s 37us/step - loss: 0.2229 - acc: 0.9381\n",
      "Epoch 86/100\n",
      "55000/55000 [==============================] - 2s 38us/step - loss: 0.2230 - acc: 0.9383\n",
      "Epoch 87/100\n",
      "55000/55000 [==============================] - 2s 39us/step - loss: 0.2227 - acc: 0.9379\n",
      "Epoch 88/100\n",
      "55000/55000 [==============================] - 2s 41us/step - loss: 0.2227 - acc: 0.9378\n",
      "Epoch 89/100\n",
      "55000/55000 [==============================] - 2s 41us/step - loss: 0.2228 - acc: 0.9380\n",
      "Epoch 90/100\n",
      "55000/55000 [==============================] - 2s 42us/step - loss: 0.2223 - acc: 0.9382\n",
      "Epoch 91/100\n",
      "55000/55000 [==============================] - 2s 39us/step - loss: 0.2224 - acc: 0.9383\n",
      "Epoch 92/100\n",
      "55000/55000 [==============================] - 2s 38us/step - loss: 0.2220 - acc: 0.9378\n",
      "Epoch 93/100\n",
      "55000/55000 [==============================] - 2s 39us/step - loss: 0.2218 - acc: 0.9386\n",
      "Epoch 94/100\n",
      "55000/55000 [==============================] - 2s 38us/step - loss: 0.2220 - acc: 0.9385\n",
      "Epoch 95/100\n",
      "55000/55000 [==============================] - 2s 38us/step - loss: 0.2219 - acc: 0.9386\n",
      "Epoch 96/100\n",
      "55000/55000 [==============================] - 2s 38us/step - loss: 0.2215 - acc: 0.9384\n",
      "Epoch 97/100\n",
      "55000/55000 [==============================] - 2s 38us/step - loss: 0.2212 - acc: 0.9386\n",
      "Epoch 98/100\n",
      "55000/55000 [==============================] - 2s 39us/step - loss: 0.2213 - acc: 0.9386\n",
      "Epoch 99/100\n",
      "55000/55000 [==============================] - 2s 41us/step - loss: 0.2213 - acc: 0.9387\n",
      "Epoch 100/100\n",
      "55000/55000 [==============================] - 2s 43us/step - loss: 0.2211 - acc: 0.9390\n",
      "55000/55000 [==============================] - 4s 67us/step\n",
      "training accuracy 0.940509090909091\n",
      "10000/10000 [==============================] - 1s 62us/step\n",
      "test accuracy 0.9278\n"
     ]
    }
   ],
   "source": [
    "# Softmax Regression\n",
    "\"\"\"\n",
    "The simplest approach to a classification problem with more than 2 categories is to apply softmax\n",
    "regression. Since softmax regression is essentially a single-layer neural network, it is expected\n",
    "to perform poorly. \n",
    "\"\"\"\n",
    "\n",
    "# Create the softmax regression model\n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(units = 10, input_dim = 784, activation = 'softmax', name = 'dense'))\n",
    "\n",
    "# Compile\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fit\n",
    "classifier.fit(X_train, y_train, batch_size = 128, epochs = 100)\n",
    "\n",
    "# Accuracy on the training subset\n",
    "print('training accuracy', classifier.evaluate(X_train, y_train)[1])\n",
    "\n",
    "# Accuracy on the test set\n",
    "print('test accuracy', classifier.evaluate(X_test, y_test)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summary Softmax Regression\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "55000/55000 [==============================] - 3s 54us/step - loss: 0.5102 - acc: 0.8555\n",
      "Epoch 2/100\n",
      "55000/55000 [==============================] - 3s 48us/step - loss: 0.2312 - acc: 0.9342\n",
      "Epoch 3/100\n",
      "55000/55000 [==============================] - 3s 48us/step - loss: 0.1861 - acc: 0.9459\n",
      "Epoch 4/100\n",
      "55000/55000 [==============================] - 3s 48us/step - loss: 0.1591 - acc: 0.9530\n",
      "Epoch 5/100\n",
      "55000/55000 [==============================] - 3s 48us/step - loss: 0.1394 - acc: 0.9585\n",
      "Epoch 6/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.1251 - acc: 0.9622\n",
      "Epoch 7/100\n",
      "55000/55000 [==============================] - 3s 47us/step - loss: 0.1141 - acc: 0.9660\n",
      "Epoch 8/100\n",
      "55000/55000 [==============================] - 3s 50us/step - loss: 0.1034 - acc: 0.9697\n",
      "Epoch 9/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0960 - acc: 0.9716\n",
      "Epoch 10/100\n",
      "55000/55000 [==============================] - 3s 56us/step - loss: 0.0876 - acc: 0.9737\n",
      "Epoch 11/100\n",
      "55000/55000 [==============================] - 3s 51us/step - loss: 0.0820 - acc: 0.9753\n",
      "Epoch 12/100\n",
      "55000/55000 [==============================] - 3s 54us/step - loss: 0.0755 - acc: 0.9776\n",
      "Epoch 13/100\n",
      "55000/55000 [==============================] - 3s 51us/step - loss: 0.0719 - acc: 0.9782\n",
      "Epoch 14/100\n",
      "55000/55000 [==============================] - 3s 54us/step - loss: 0.0668 - acc: 0.9794\n",
      "Epoch 15/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0627 - acc: 0.9810\n",
      "Epoch 16/100\n",
      "55000/55000 [==============================] - 3s 53us/step - loss: 0.0598 - acc: 0.9813\n",
      "Epoch 17/100\n",
      "55000/55000 [==============================] - 3s 53us/step - loss: 0.0561 - acc: 0.9831\n",
      "Epoch 18/100\n",
      "55000/55000 [==============================] - 3s 50us/step - loss: 0.0534 - acc: 0.9841\n",
      "Epoch 19/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0512 - acc: 0.9845\n",
      "Epoch 20/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0467 - acc: 0.9853\n",
      "Epoch 21/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0447 - acc: 0.9858\n",
      "Epoch 22/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0409 - acc: 0.9881\n",
      "Epoch 23/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0399 - acc: 0.9876\n",
      "Epoch 24/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0380 - acc: 0.9884\n",
      "Epoch 25/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0352 - acc: 0.9889\n",
      "Epoch 26/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0331 - acc: 0.9897\n",
      "Epoch 27/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0316 - acc: 0.9903\n",
      "Epoch 28/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0297 - acc: 0.9908\n",
      "Epoch 29/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0281 - acc: 0.9912\n",
      "Epoch 30/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0265 - acc: 0.9915\n",
      "Epoch 31/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0260 - acc: 0.9921\n",
      "Epoch 32/100\n",
      "55000/55000 [==============================] - 3s 48us/step - loss: 0.0248 - acc: 0.9924\n",
      "Epoch 33/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0231 - acc: 0.9929\n",
      "Epoch 34/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0218 - acc: 0.9932\n",
      "Epoch 35/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0216 - acc: 0.9931\n",
      "Epoch 36/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0185 - acc: 0.9945\n",
      "Epoch 37/100\n",
      "55000/55000 [==============================] - 3s 48us/step - loss: 0.0187 - acc: 0.9939\n",
      "Epoch 38/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0174 - acc: 0.9945\n",
      "Epoch 39/100\n",
      "55000/55000 [==============================] - 3s 48us/step - loss: 0.0150 - acc: 0.9955\n",
      "Epoch 40/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0161 - acc: 0.9952\n",
      "Epoch 41/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0159 - acc: 0.9950\n",
      "Epoch 42/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0132 - acc: 0.9960\n",
      "Epoch 43/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0131 - acc: 0.9963\n",
      "Epoch 44/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0127 - acc: 0.9962\n",
      "Epoch 45/100\n",
      "55000/55000 [==============================] - 3s 48us/step - loss: 0.0134 - acc: 0.9957\n",
      "Epoch 46/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0107 - acc: 0.9972\n",
      "Epoch 47/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0088 - acc: 0.9980\n",
      "Epoch 48/100\n",
      "55000/55000 [==============================] - 3s 48us/step - loss: 0.0135 - acc: 0.9956\n",
      "Epoch 49/100\n",
      "55000/55000 [==============================] - 3s 48us/step - loss: 0.0104 - acc: 0.9969\n",
      "Epoch 50/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0087 - acc: 0.9974\n",
      "Epoch 51/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0083 - acc: 0.9979\n",
      "Epoch 52/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0110 - acc: 0.9965\n",
      "Epoch 53/100\n",
      "55000/55000 [==============================] - 3s 48us/step - loss: 0.0076 - acc: 0.9978\n",
      "Epoch 54/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0074 - acc: 0.9980\n",
      "Epoch 55/100\n",
      "55000/55000 [==============================] - 3s 48us/step - loss: 0.0087 - acc: 0.9975\n",
      "Epoch 56/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0088 - acc: 0.9970\n",
      "Epoch 57/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0068 - acc: 0.9981\n",
      "Epoch 58/100\n",
      "55000/55000 [==============================] - 3s 48us/step - loss: 0.0059 - acc: 0.9985\n",
      "Epoch 59/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0082 - acc: 0.9977\n",
      "Epoch 60/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0056 - acc: 0.9986\n",
      "Epoch 61/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0066 - acc: 0.9981\n",
      "Epoch 62/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0072 - acc: 0.9979\n",
      "Epoch 63/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0080 - acc: 0.9974\n",
      "Epoch 64/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0046 - acc: 0.9989\n",
      "Epoch 65/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0037 - acc: 0.9993\n",
      "Epoch 66/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0017 - acc: 0.9999\n",
      "Epoch 67/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0116 - acc: 0.9965\n",
      "Epoch 69/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0159 - acc: 0.9945\n",
      "Epoch 70/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0034 - acc: 0.9992\n",
      "Epoch 71/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0021 - acc: 0.9998\n",
      "Epoch 72/100\n",
      "55000/55000 [==============================] - 3s 50us/step - loss: 0.0014 - acc: 0.9999\n",
      "Epoch 73/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "55000/55000 [==============================] - 3s 52us/step - loss: 7.1145e-04 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "55000/55000 [==============================] - 3s 51us/step - loss: 9.2794e-04 - acc: 0.9999\n",
      "Epoch 76/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0244 - acc: 0.9924\n",
      "Epoch 77/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0061 - acc: 0.9982\n",
      "Epoch 78/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0029 - acc: 0.9993\n",
      "Epoch 79/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0024 - acc: 0.9995\n",
      "Epoch 80/100\n",
      "55000/55000 [==============================] - 3s 49us/step - loss: 0.0024 - acc: 0.9995\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55000/55000 [==============================] - 3s 48us/step - loss: 0.0011 - acc: 0.9999\n",
      "Epoch 82/100\n",
      "55000/55000 [==============================] - 3s 48us/step - loss: 7.1561e-04 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "55000/55000 [==============================] - 3s 48us/step - loss: 4.8545e-04 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "55000/55000 [==============================] - 3s 48us/step - loss: 4.5895e-04 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "55000/55000 [==============================] - 3s 48us/step - loss: 4.5257e-04 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "55000/55000 [==============================] - 3s 48us/step - loss: 0.0021 - acc: 0.9994\n",
      "Epoch 87/100\n",
      "55000/55000 [==============================] - 3s 48us/step - loss: 0.0328 - acc: 0.9911\n",
      "Epoch 88/100\n",
      "55000/55000 [==============================] - 3s 48us/step - loss: 0.0043 - acc: 0.9987\n",
      "Epoch 89/100\n",
      "55000/55000 [==============================] - 3s 48us/step - loss: 0.0026 - acc: 0.9993\n",
      "Epoch 90/100\n",
      "55000/55000 [==============================] - 3s 48us/step - loss: 0.0012 - acc: 0.9998\n",
      "Epoch 91/100\n",
      "55000/55000 [==============================] - 3s 48us/step - loss: 5.0642e-04 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "55000/55000 [==============================] - 3s 48us/step - loss: 3.9520e-04 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "55000/55000 [==============================] - 3s 48us/step - loss: 3.4251e-04 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "55000/55000 [==============================] - 3s 48us/step - loss: 3.0190e-04 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "55000/55000 [==============================] - 3s 48us/step - loss: 3.4418e-04 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "55000/55000 [==============================] - 3s 48us/step - loss: 0.0281 - acc: 0.9920\n",
      "Epoch 97/100\n",
      "55000/55000 [==============================] - 3s 48us/step - loss: 0.0047 - acc: 0.9985\n",
      "Epoch 98/100\n",
      "55000/55000 [==============================] - 3s 50us/step - loss: 0.0026 - acc: 0.9992\n",
      "Epoch 99/100\n",
      "55000/55000 [==============================] - 3s 48us/step - loss: 7.8548e-04 - acc: 0.9999\n",
      "Epoch 100/100\n",
      "55000/55000 [==============================] - 3s 48us/step - loss: 3.7186e-04 - acc: 1.0000\n",
      "55000/55000 [==============================] - 3s 58us/step\n",
      "training accuracy 1.0\n",
      "10000/10000 [==============================] - 1s 58us/step\n",
      "test accuracy 0.9675\n"
     ]
    }
   ],
   "source": [
    "# 3-Layer Standard Neural Network\n",
    "\"\"\"\n",
    "A 3-layer neural network, although not deep, usually has enough layers to outperform traditional \n",
    "Machine Learning algorithms. These extra layers allow the network to learn more complex features \n",
    "that can be useful for classification. \n",
    "\"\"\"\n",
    "\n",
    "# Create the 3-layer neural network\n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(units = 32, input_dim = 784, activation = 'relu', name = 'dense_1'))\n",
    "classifier.add(Dense(units = 32, activation = 'relu', name = 'dense_2'))\n",
    "classifier.add(Dense(units = 10, activation = 'softmax', name = 'dense_3'))\n",
    "\n",
    "# Compile\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fit\n",
    "classifier.fit(X_train, y_train, batch_size = 128, epochs = 100)\n",
    "\n",
    "# Accuracy on the training subset\n",
    "print('training accuracy', classifier.evaluate(X_train, y_train)[1])\n",
    "\n",
    "# Accuracy on the test set\n",
    "print('test accuracy', classifier.evaluate(X_test, y_test)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 26,506\n",
      "Trainable params: 26,506\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summary 3-Layer Standard Neural Network\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "55000/55000 [==============================] - 5s 90us/step - loss: 1.2092 - acc: 0.5538\n",
      "Epoch 2/100\n",
      "55000/55000 [==============================] - 4s 73us/step - loss: 0.5473 - acc: 0.8298\n",
      "Epoch 3/100\n",
      "55000/55000 [==============================] - 4s 74us/step - loss: 0.4074 - acc: 0.8870\n",
      "Epoch 4/100\n",
      "55000/55000 [==============================] - 4s 73us/step - loss: 0.3286 - acc: 0.9110\n",
      "Epoch 5/100\n",
      "55000/55000 [==============================] - 4s 73us/step - loss: 0.2909 - acc: 0.9225\n",
      "Epoch 6/100\n",
      "55000/55000 [==============================] - 4s 73us/step - loss: 0.2639 - acc: 0.9303\n",
      "Epoch 7/100\n",
      "55000/55000 [==============================] - 4s 73us/step - loss: 0.2386 - acc: 0.9379\n",
      "Epoch 8/100\n",
      "55000/55000 [==============================] - 4s 73us/step - loss: 0.2210 - acc: 0.9423\n",
      "Epoch 9/100\n",
      "55000/55000 [==============================] - 4s 74us/step - loss: 0.2040 - acc: 0.9470\n",
      "Epoch 10/100\n",
      "55000/55000 [==============================] - 4s 73us/step - loss: 0.1878 - acc: 0.9522\n",
      "Epoch 11/100\n",
      "55000/55000 [==============================] - 4s 74us/step - loss: 0.1820 - acc: 0.9521\n",
      "Epoch 12/100\n",
      "55000/55000 [==============================] - 4s 75us/step - loss: 0.1697 - acc: 0.9565\n",
      "Epoch 13/100\n",
      "55000/55000 [==============================] - 4s 75us/step - loss: 0.1668 - acc: 0.9578\n",
      "Epoch 14/100\n",
      "55000/55000 [==============================] - 4s 74us/step - loss: 0.1542 - acc: 0.9614\n",
      "Epoch 15/100\n",
      "55000/55000 [==============================] - 4s 75us/step - loss: 0.1543 - acc: 0.9612\n",
      "Epoch 16/100\n",
      "55000/55000 [==============================] - 4s 74us/step - loss: 0.1455 - acc: 0.9634\n",
      "Epoch 17/100\n",
      "55000/55000 [==============================] - 4s 75us/step - loss: 0.1396 - acc: 0.9643\n",
      "Epoch 18/100\n",
      "55000/55000 [==============================] - 4s 75us/step - loss: 0.1307 - acc: 0.9669\n",
      "Epoch 19/100\n",
      "55000/55000 [==============================] - 4s 75us/step - loss: 0.1306 - acc: 0.9663\n",
      "Epoch 20/100\n",
      "55000/55000 [==============================] - 4s 74us/step - loss: 0.1244 - acc: 0.9682\n",
      "Epoch 21/100\n",
      "55000/55000 [==============================] - 4s 75us/step - loss: 0.1218 - acc: 0.9701\n",
      "Epoch 22/100\n",
      "55000/55000 [==============================] - 4s 77us/step - loss: 0.1174 - acc: 0.9699\n",
      "Epoch 23/100\n",
      "55000/55000 [==============================] - 4s 75us/step - loss: 0.1120 - acc: 0.9709\n",
      "Epoch 24/100\n",
      "55000/55000 [==============================] - 4s 75us/step - loss: 0.1142 - acc: 0.9709\n",
      "Epoch 25/100\n",
      "55000/55000 [==============================] - 4s 78us/step - loss: 0.1090 - acc: 0.9724\n",
      "Epoch 26/100\n",
      "55000/55000 [==============================] - 4s 76us/step - loss: 0.1038 - acc: 0.9731\n",
      "Epoch 27/100\n",
      "55000/55000 [==============================] - 4s 77us/step - loss: 0.1033 - acc: 0.9733\n",
      "Epoch 28/100\n",
      "55000/55000 [==============================] - 5s 88us/step - loss: 0.0985 - acc: 0.9751\n",
      "Epoch 29/100\n",
      "55000/55000 [==============================] - 4s 81us/step - loss: 0.0974 - acc: 0.9748\n",
      "Epoch 30/100\n",
      "55000/55000 [==============================] - 4s 79us/step - loss: 0.0939 - acc: 0.9769\n",
      "Epoch 31/100\n",
      "55000/55000 [==============================] - 4s 79us/step - loss: 0.0980 - acc: 0.9757\n",
      "Epoch 32/100\n",
      "55000/55000 [==============================] - 4s 78us/step - loss: 0.0954 - acc: 0.9764\n",
      "Epoch 33/100\n",
      "55000/55000 [==============================] - 4s 76us/step - loss: 0.0905 - acc: 0.9777\n",
      "Epoch 34/100\n",
      "55000/55000 [==============================] - 4s 78us/step - loss: 0.0879 - acc: 0.9779\n",
      "Epoch 35/100\n",
      "55000/55000 [==============================] - 4s 72us/step - loss: 0.0866 - acc: 0.9780\n",
      "Epoch 36/100\n",
      "55000/55000 [==============================] - 4s 70us/step - loss: 0.0863 - acc: 0.9783\n",
      "Epoch 37/100\n",
      "55000/55000 [==============================] - 4s 70us/step - loss: 0.0818 - acc: 0.9798\n",
      "Epoch 38/100\n",
      "55000/55000 [==============================] - 4s 71us/step - loss: 0.0843 - acc: 0.9785\n",
      "Epoch 39/100\n",
      "55000/55000 [==============================] - 4s 71us/step - loss: 0.0812 - acc: 0.9793\n",
      "Epoch 40/100\n",
      "55000/55000 [==============================] - 4s 69us/step - loss: 0.0748 - acc: 0.9809\n",
      "Epoch 41/100\n",
      "55000/55000 [==============================] - 4s 71us/step - loss: 0.0775 - acc: 0.9801\n",
      "Epoch 42/100\n",
      "55000/55000 [==============================] - 4s 70us/step - loss: 0.0772 - acc: 0.9805\n",
      "Epoch 43/100\n",
      "55000/55000 [==============================] - 4s 70us/step - loss: 0.0784 - acc: 0.9806\n",
      "Epoch 44/100\n",
      "55000/55000 [==============================] - 4s 70us/step - loss: 0.0728 - acc: 0.9818\n",
      "Epoch 45/100\n",
      "55000/55000 [==============================] - 4s 70us/step - loss: 0.0714 - acc: 0.9821\n",
      "Epoch 46/100\n",
      "55000/55000 [==============================] - 4s 71us/step - loss: 0.0741 - acc: 0.9816\n",
      "Epoch 47/100\n",
      "55000/55000 [==============================] - 4s 70us/step - loss: 0.0719 - acc: 0.9817\n",
      "Epoch 48/100\n",
      "55000/55000 [==============================] - 4s 71us/step - loss: 0.0724 - acc: 0.9819\n",
      "Epoch 49/100\n",
      "55000/55000 [==============================] - 4s 77us/step - loss: 0.0690 - acc: 0.9826\n",
      "Epoch 50/100\n",
      "55000/55000 [==============================] - 4s 75us/step - loss: 0.0666 - acc: 0.9835\n",
      "Epoch 51/100\n",
      "55000/55000 [==============================] - 4s 74us/step - loss: 0.0678 - acc: 0.9834\n",
      "Epoch 52/100\n",
      "55000/55000 [==============================] - 4s 74us/step - loss: 0.0670 - acc: 0.9835\n",
      "Epoch 53/100\n",
      "55000/55000 [==============================] - 4s 79us/step - loss: 0.0689 - acc: 0.9827\n",
      "Epoch 54/100\n",
      "55000/55000 [==============================] - 4s 79us/step - loss: 0.0645 - acc: 0.9839\n",
      "Epoch 55/100\n",
      "55000/55000 [==============================] - 4s 81us/step - loss: 0.0653 - acc: 0.9835\n",
      "Epoch 56/100\n",
      "55000/55000 [==============================] - 4s 82us/step - loss: 0.0694 - acc: 0.9831\n",
      "Epoch 57/100\n",
      "55000/55000 [==============================] - 4s 76us/step - loss: 0.0580 - acc: 0.9851\n",
      "Epoch 58/100\n",
      "55000/55000 [==============================] - 4s 78us/step - loss: 0.0688 - acc: 0.9829\n",
      "Epoch 59/100\n",
      "55000/55000 [==============================] - 4s 77us/step - loss: 0.0644 - acc: 0.9837\n",
      "Epoch 60/100\n",
      "55000/55000 [==============================] - 4s 77us/step - loss: 0.0596 - acc: 0.9855\n",
      "Epoch 61/100\n",
      "55000/55000 [==============================] - 4s 76us/step - loss: 0.0581 - acc: 0.9850\n",
      "Epoch 62/100\n",
      "55000/55000 [==============================] - 4s 76us/step - loss: 0.0587 - acc: 0.9858\n",
      "Epoch 63/100\n",
      "55000/55000 [==============================] - 4s 77us/step - loss: 0.0628 - acc: 0.9848\n",
      "Epoch 64/100\n",
      "55000/55000 [==============================] - 4s 76us/step - loss: 0.0599 - acc: 0.9853\n",
      "Epoch 65/100\n",
      "55000/55000 [==============================] - 4s 76us/step - loss: 0.0576 - acc: 0.9849\n",
      "Epoch 66/100\n",
      "55000/55000 [==============================] - 4s 78us/step - loss: 0.0535 - acc: 0.9866\n",
      "Epoch 67/100\n",
      "55000/55000 [==============================] - 4s 79us/step - loss: 0.0635 - acc: 0.9838\n",
      "Epoch 68/100\n",
      "55000/55000 [==============================] - 4s 80us/step - loss: 0.0540 - acc: 0.9859\n",
      "Epoch 69/100\n",
      "55000/55000 [==============================] - 4s 78us/step - loss: 0.0545 - acc: 0.9863\n",
      "Epoch 70/100\n",
      "55000/55000 [==============================] - 4s 78us/step - loss: 0.0557 - acc: 0.9861\n",
      "Epoch 71/100\n",
      "55000/55000 [==============================] - 4s 79us/step - loss: 0.0589 - acc: 0.9854\n",
      "Epoch 72/100\n",
      "55000/55000 [==============================] - 4s 78us/step - loss: 0.0486 - acc: 0.9880\n",
      "Epoch 73/100\n",
      "55000/55000 [==============================] - 4s 80us/step - loss: 0.0581 - acc: 0.9854\n",
      "Epoch 74/100\n",
      "55000/55000 [==============================] - 4s 78us/step - loss: 0.0551 - acc: 0.9867\n",
      "Epoch 75/100\n",
      "55000/55000 [==============================] - 4s 78us/step - loss: 0.0513 - acc: 0.9871\n",
      "Epoch 76/100\n",
      "55000/55000 [==============================] - 4s 79us/step - loss: 0.0525 - acc: 0.9865\n",
      "Epoch 77/100\n",
      "55000/55000 [==============================] - 4s 80us/step - loss: 0.0546 - acc: 0.9868\n",
      "Epoch 78/100\n",
      "55000/55000 [==============================] - 4s 78us/step - loss: 0.0499 - acc: 0.9871\n",
      "Epoch 79/100\n",
      "55000/55000 [==============================] - 4s 78us/step - loss: 0.0525 - acc: 0.9868\n",
      "Epoch 80/100\n",
      "55000/55000 [==============================] - 4s 78us/step - loss: 0.0470 - acc: 0.9884\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55000/55000 [==============================] - 4s 78us/step - loss: 0.0570 - acc: 0.9858\n",
      "Epoch 82/100\n",
      "55000/55000 [==============================] - 4s 79us/step - loss: 0.0514 - acc: 0.9871\n",
      "Epoch 83/100\n",
      "55000/55000 [==============================] - 4s 79us/step - loss: 0.0541 - acc: 0.9869\n",
      "Epoch 84/100\n",
      "55000/55000 [==============================] - 4s 79us/step - loss: 0.0486 - acc: 0.9877\n",
      "Epoch 85/100\n",
      "55000/55000 [==============================] - 4s 79us/step - loss: 0.0499 - acc: 0.9880\n",
      "Epoch 86/100\n",
      "55000/55000 [==============================] - 4s 80us/step - loss: 0.0510 - acc: 0.9875\n",
      "Epoch 87/100\n",
      "55000/55000 [==============================] - 4s 78us/step - loss: 0.0492 - acc: 0.9878\n",
      "Epoch 88/100\n",
      "55000/55000 [==============================] - 4s 78us/step - loss: 0.0457 - acc: 0.9887\n",
      "Epoch 89/100\n",
      "55000/55000 [==============================] - 4s 79us/step - loss: 0.0444 - acc: 0.9892\n",
      "Epoch 90/100\n",
      "55000/55000 [==============================] - 4s 79us/step - loss: 0.0491 - acc: 0.9880\n",
      "Epoch 91/100\n",
      "55000/55000 [==============================] - 4s 79us/step - loss: 0.0498 - acc: 0.9876\n",
      "Epoch 92/100\n",
      "55000/55000 [==============================] - 4s 80us/step - loss: 0.0464 - acc: 0.9891\n",
      "Epoch 93/100\n",
      "55000/55000 [==============================] - 4s 78us/step - loss: 0.0469 - acc: 0.9886\n",
      "Epoch 94/100\n",
      "55000/55000 [==============================] - 4s 79us/step - loss: 0.0547 - acc: 0.9866\n",
      "Epoch 95/100\n",
      "55000/55000 [==============================] - 4s 80us/step - loss: 0.0465 - acc: 0.9886\n",
      "Epoch 96/100\n",
      "55000/55000 [==============================] - 4s 80us/step - loss: 0.0501 - acc: 0.9882\n",
      "Epoch 97/100\n",
      "55000/55000 [==============================] - 4s 79us/step - loss: 0.0473 - acc: 0.9879\n",
      "Epoch 98/100\n",
      "55000/55000 [==============================] - 4s 79us/step - loss: 0.0414 - acc: 0.9893\n",
      "Epoch 99/100\n",
      "55000/55000 [==============================] - 4s 80us/step - loss: 0.0519 - acc: 0.9872\n",
      "Epoch 100/100\n",
      "55000/55000 [==============================] - 4s 80us/step - loss: 0.0490 - acc: 0.9881\n",
      "55000/55000 [==============================] - 4s 73us/step\n",
      "training accuracy 0.9951454545454546\n",
      "10000/10000 [==============================] - 1s 72us/step\n",
      "test accuracy 0.971\n"
     ]
    }
   ],
   "source": [
    "# 8-Layer Standard Neural Network\n",
    "\"\"\"\n",
    "An 8-layer neural network is considered a deep network. Because it has more layers it should be \n",
    "able to learn even more complex features that can be useful for classification, however, since\n",
    "it is deeper it is more prone to overfitting which is why we apply some regularization.\n",
    "\"\"\"\n",
    "\n",
    "# Create the 8-layer neural network\n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(units = 32, input_dim = 784, activation = 'relu', name = 'dense_1'))\n",
    "classifier.add(Dense(units = 32, activation = 'relu', name = 'dense_2'))\n",
    "classifier.add(Dense(units = 32, activation = 'relu', name = 'dense_3'))\n",
    "classifier.add(Dense(units = 32, activation = 'relu', name = 'dense_4'))\n",
    "classifier.add(Dense(units = 32, activation = 'relu', name = 'dense_5'))\n",
    "classifier.add(Dropout(rate = 0.3, name = 'dropout_5'))\n",
    "classifier.add(Dense(units = 32, activation = 'relu', name = 'dense_6'))\n",
    "classifier.add(Dropout(rate = 0.3, name = 'dropout_6'))\n",
    "classifier.add(Dense(units = 32, activation = 'relu', name = 'dense_7'))\n",
    "classifier.add(Dropout(rate = 0.3, name = 'dropout_7'))\n",
    "classifier.add(Dense(units = 10, activation = 'softmax', name = 'dense_8'))\n",
    "\n",
    "# Compile\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fit\n",
    "classifier.fit(X_train, y_train, batch_size = 128, epochs = 100)\n",
    "\n",
    "# Accuracy on the training subset\n",
    "print('training accuracy', classifier.evaluate(X_train, y_train)[1])\n",
    "\n",
    "# Accuracy on the test set\n",
    "print('test accuracy', classifier.evaluate(X_test, y_test)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 31,786\n",
      "Trainable params: 31,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summary 8-Layer Standard Neural Network\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "55000/55000 [==============================] - 6s 109us/step - loss: 0.2425 - acc: 0.9303\n",
      "Epoch 2/25\n",
      "55000/55000 [==============================] - 6s 100us/step - loss: 0.0651 - acc: 0.9797\n",
      "Epoch 3/25\n",
      "55000/55000 [==============================] - 6s 102us/step - loss: 0.0464 - acc: 0.9859\n",
      "Epoch 4/25\n",
      "55000/55000 [==============================] - 6s 104us/step - loss: 0.0379 - acc: 0.9882\n",
      "Epoch 5/25\n",
      "55000/55000 [==============================] - 6s 100us/step - loss: 0.0299 - acc: 0.9907\n",
      "Epoch 6/25\n",
      "55000/55000 [==============================] - 6s 100us/step - loss: 0.0253 - acc: 0.9921\n",
      "Epoch 7/25\n",
      "55000/55000 [==============================] - 6s 100us/step - loss: 0.0220 - acc: 0.9933\n",
      "Epoch 8/25\n",
      "55000/55000 [==============================] - 6s 101us/step - loss: 0.0187 - acc: 0.9939\n",
      "Epoch 9/25\n",
      "55000/55000 [==============================] - 6s 101us/step - loss: 0.0147 - acc: 0.9953\n",
      "Epoch 10/25\n",
      "55000/55000 [==============================] - 6s 101us/step - loss: 0.0144 - acc: 0.9953\n",
      "Epoch 11/25\n",
      "55000/55000 [==============================] - 6s 102us/step - loss: 0.0112 - acc: 0.9963\n",
      "Epoch 12/25\n",
      "55000/55000 [==============================] - 6s 102us/step - loss: 0.0105 - acc: 0.9966\n",
      "Epoch 13/25\n",
      "55000/55000 [==============================] - 6s 102us/step - loss: 0.0087 - acc: 0.9972\n",
      "Epoch 14/25\n",
      "55000/55000 [==============================] - 6s 102us/step - loss: 0.0072 - acc: 0.9977\n",
      "Epoch 15/25\n",
      "55000/55000 [==============================] - 6s 103us/step - loss: 0.0080 - acc: 0.9971\n",
      "Epoch 16/25\n",
      "55000/55000 [==============================] - 6s 102us/step - loss: 0.0049 - acc: 0.9984\n",
      "Epoch 17/25\n",
      "55000/55000 [==============================] - 6s 103us/step - loss: 0.0054 - acc: 0.9981\n",
      "Epoch 18/25\n",
      "55000/55000 [==============================] - 6s 103us/step - loss: 0.0066 - acc: 0.9977\n",
      "Epoch 19/25\n",
      "55000/55000 [==============================] - 6s 102us/step - loss: 0.0051 - acc: 0.9983\n",
      "Epoch 20/25\n",
      "55000/55000 [==============================] - 6s 103us/step - loss: 0.0041 - acc: 0.9986\n",
      "Epoch 21/25\n",
      "55000/55000 [==============================] - 6s 103us/step - loss: 0.0033 - acc: 0.9989\n",
      "Epoch 22/25\n",
      "55000/55000 [==============================] - 6s 102us/step - loss: 0.0045 - acc: 0.9986\n",
      "Epoch 23/25\n",
      "55000/55000 [==============================] - 6s 102us/step - loss: 0.0018 - acc: 0.9997\n",
      "Epoch 24/25\n",
      "55000/55000 [==============================] - 6s 102us/step - loss: 0.0044 - acc: 0.9985\n",
      "Epoch 25/25\n",
      "55000/55000 [==============================] - 6s 103us/step - loss: 0.0046 - acc: 0.9985\n",
      "55000/55000 [==============================] - 6s 109us/step\n",
      "training accuracy 0.9991636363636364\n",
      "10000/10000 [==============================] - 1s 104us/step\n",
      "test accuracy 0.9909\n"
     ]
    }
   ],
   "source": [
    "# 3-layer ConvNet\n",
    "\"\"\"\n",
    "A Convolutional neural network performs much better than standard neural networks when it comes to \n",
    "computer vision tasks. This is because it tries to learn features or rather feature detectors from \n",
    "batches of pixels, taking into account the shape of the image. The parameters learned come in the \n",
    "form of filters. There are also considerable fewer parameters, which result from parameter sharing \n",
    "and sparsity of connections. This particular ConvNet has 2 convolutional layer and 1 fully-connected \n",
    "layers.\n",
    "\"\"\"\n",
    "\n",
    "# Create the 3-layer CNN\n",
    "classifier = Sequential()\n",
    "classifier.add(Conv2D(64, (5, 5), input_shape = (28, 28, 1), activation = 'relu', name = 'conv2d_1'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2), name = 'maxpool2d_1'))\n",
    "classifier.add(Conv2D(64, (3, 3), activation = 'relu', name = 'conv2d_2'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2), name = 'maxpool2d_2'))\n",
    "classifier.add(Flatten(name = 'flatten_2'))\n",
    "classifier.add(Dense(units = 10, activation = 'softmax', name = 'dense_3'))\n",
    "\n",
    "# Compile\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fit\n",
    "classifier.fit(X_train_image, y_train, batch_size = 128, epochs = 25)\n",
    "\n",
    "# Accuracy on the training subset\n",
    "print('training accuracy', classifier.evaluate(X_train_image, y_train)[1])\n",
    "\n",
    "# Accuracy on the test set\n",
    "print('test accuracy', classifier.evaluate(X_test_image, y_test)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 64)        1664      \n",
      "_________________________________________________________________\n",
      "maxpool2d_1 (MaxPooling2D)   (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 10, 10, 64)        36928     \n",
      "_________________________________________________________________\n",
      "maxpool2d_2 (MaxPooling2D)   (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                16010     \n",
      "=================================================================\n",
      "Total params: 54,602\n",
      "Trainable params: 54,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summary 3-Layer ConvNet\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "55000/55000 [==============================] - 11s 194us/step - loss: 0.3596 - acc: 0.8842\n",
      "Epoch 2/50\n",
      "55000/55000 [==============================] - 9s 166us/step - loss: 0.0937 - acc: 0.9731\n",
      "Epoch 3/50\n",
      "55000/55000 [==============================] - 9s 166us/step - loss: 0.0664 - acc: 0.9795\n",
      "Epoch 4/50\n",
      "55000/55000 [==============================] - 9s 165us/step - loss: 0.0537 - acc: 0.9848\n",
      "Epoch 5/50\n",
      "55000/55000 [==============================] - 9s 165us/step - loss: 0.0460 - acc: 0.9861\n",
      "Epoch 6/50\n",
      "55000/55000 [==============================] - 9s 165us/step - loss: 0.0370 - acc: 0.9892\n",
      "Epoch 7/50\n",
      "55000/55000 [==============================] - 9s 164us/step - loss: 0.0360 - acc: 0.9895\n",
      "Epoch 8/50\n",
      "55000/55000 [==============================] - 9s 166us/step - loss: 0.0319 - acc: 0.9903\n",
      "Epoch 9/50\n",
      "55000/55000 [==============================] - 9s 164us/step - loss: 0.0287 - acc: 0.9915\n",
      "Epoch 10/50\n",
      "55000/55000 [==============================] - 9s 164us/step - loss: 0.0255 - acc: 0.9926\n",
      "Epoch 11/50\n",
      "55000/55000 [==============================] - 9s 169us/step - loss: 0.0236 - acc: 0.9927\n",
      "Epoch 12/50\n",
      "55000/55000 [==============================] - 9s 167us/step - loss: 0.0224 - acc: 0.9933\n",
      "Epoch 13/50\n",
      "55000/55000 [==============================] - 9s 163us/step - loss: 0.0233 - acc: 0.9928\n",
      "Epoch 14/50\n",
      "55000/55000 [==============================] - 9s 163us/step - loss: 0.0181 - acc: 0.9947\n",
      "Epoch 15/50\n",
      "55000/55000 [==============================] - 9s 164us/step - loss: 0.0171 - acc: 0.9949\n",
      "Epoch 16/50\n",
      "55000/55000 [==============================] - 9s 164us/step - loss: 0.0170 - acc: 0.9951\n",
      "Epoch 17/50\n",
      "55000/55000 [==============================] - 9s 164us/step - loss: 0.0134 - acc: 0.9957\n",
      "Epoch 18/50\n",
      "55000/55000 [==============================] - 9s 165us/step - loss: 0.0148 - acc: 0.9954\n",
      "Epoch 19/50\n",
      "55000/55000 [==============================] - 9s 165us/step - loss: 0.0150 - acc: 0.9954\n",
      "Epoch 20/50\n",
      "55000/55000 [==============================] - 9s 170us/step - loss: 0.0137 - acc: 0.9959\n",
      "Epoch 21/50\n",
      "55000/55000 [==============================] - 9s 172us/step - loss: 0.0126 - acc: 0.9961\n",
      "Epoch 22/50\n",
      "55000/55000 [==============================] - 9s 165us/step - loss: 0.0120 - acc: 0.9965\n",
      "Epoch 23/50\n",
      "55000/55000 [==============================] - 9s 163us/step - loss: 0.0120 - acc: 0.9963\n",
      "Epoch 24/50\n",
      "55000/55000 [==============================] - 9s 163us/step - loss: 0.0109 - acc: 0.9966\n",
      "Epoch 25/50\n",
      "55000/55000 [==============================] - 9s 164us/step - loss: 0.0104 - acc: 0.9969\n",
      "Epoch 26/50\n",
      "55000/55000 [==============================] - 9s 168us/step - loss: 0.0117 - acc: 0.9966\n",
      "Epoch 27/50\n",
      "55000/55000 [==============================] - 9s 164us/step - loss: 0.0092 - acc: 0.9971\n",
      "Epoch 28/50\n",
      "55000/55000 [==============================] - 9s 165us/step - loss: 0.0102 - acc: 0.9972\n",
      "Epoch 29/50\n",
      "55000/55000 [==============================] - 9s 167us/step - loss: 0.0100 - acc: 0.9971\n",
      "Epoch 30/50\n",
      "55000/55000 [==============================] - 9s 163us/step - loss: 0.0075 - acc: 0.9975\n",
      "Epoch 31/50\n",
      "55000/55000 [==============================] - 9s 160us/step - loss: 0.0074 - acc: 0.9977\n",
      "Epoch 32/50\n",
      "55000/55000 [==============================] - 9s 160us/step - loss: 0.0104 - acc: 0.9967\n",
      "Epoch 33/50\n",
      "55000/55000 [==============================] - 9s 161us/step - loss: 0.0083 - acc: 0.9979\n",
      "Epoch 34/50\n",
      "55000/55000 [==============================] - 9s 163us/step - loss: 0.0077 - acc: 0.9978\n",
      "Epoch 35/50\n",
      "55000/55000 [==============================] - 9s 161us/step - loss: 0.0060 - acc: 0.9981\n",
      "Epoch 36/50\n",
      "55000/55000 [==============================] - 9s 161us/step - loss: 0.0091 - acc: 0.9970\n",
      "Epoch 37/50\n",
      "55000/55000 [==============================] - 9s 164us/step - loss: 0.0090 - acc: 0.9972\n",
      "Epoch 38/50\n",
      "55000/55000 [==============================] - 9s 161us/step - loss: 0.0074 - acc: 0.9979\n",
      "Epoch 39/50\n",
      "55000/55000 [==============================] - 9s 161us/step - loss: 0.0056 - acc: 0.9982\n",
      "Epoch 40/50\n",
      "55000/55000 [==============================] - 9s 161us/step - loss: 0.0076 - acc: 0.9977\n",
      "Epoch 41/50\n",
      "55000/55000 [==============================] - 9s 164us/step - loss: 0.0061 - acc: 0.9980\n",
      "Epoch 42/50\n",
      "55000/55000 [==============================] - 9s 160us/step - loss: 0.0066 - acc: 0.9980\n",
      "Epoch 43/50\n",
      "55000/55000 [==============================] - 9s 159us/step - loss: 0.0059 - acc: 0.9982\n",
      "Epoch 44/50\n",
      "55000/55000 [==============================] - 9s 159us/step - loss: 0.0074 - acc: 0.9977\n",
      "Epoch 45/50\n",
      "55000/55000 [==============================] - 9s 161us/step - loss: 0.0056 - acc: 0.9981\n",
      "Epoch 46/50\n",
      "55000/55000 [==============================] - 9s 161us/step - loss: 0.0061 - acc: 0.9983\n",
      "Epoch 47/50\n",
      "55000/55000 [==============================] - 9s 162us/step - loss: 0.0064 - acc: 0.9982\n",
      "Epoch 48/50\n",
      "55000/55000 [==============================] - 9s 166us/step - loss: 0.0051 - acc: 0.9985\n",
      "Epoch 49/50\n",
      "55000/55000 [==============================] - 9s 161us/step - loss: 0.0044 - acc: 0.9988\n",
      "Epoch 50/50\n",
      "55000/55000 [==============================] - 9s 165us/step - loss: 0.0066 - acc: 0.9982\n",
      "55000/55000 [==============================] - 8s 147us/step\n",
      "training accuracy 0.9993272727272727\n",
      "10000/10000 [==============================] - 1s 147us/step\n",
      "test accuracy 0.993\n"
     ]
    }
   ],
   "source": [
    "# 8-layer ConvNet\n",
    "\"\"\"\n",
    "The deeper a Convolutional network is, the more complex features it can learn. These complex \n",
    "features may be useful in classifying an image. This particular ConvNet has 5 convolutional \n",
    "layer and 3 fully-connected layers.\n",
    "\"\"\"\n",
    "\n",
    "# Create the 8-layer CNN\n",
    "classifier = Sequential()\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (28, 28, 1), activation = 'relu', name = 'conv2d_1'))\n",
    "classifier.add(Conv2D(32, (3, 3), activation = 'relu', name = 'conv2d_2'))\n",
    "classifier.add(Conv2D(32, (3, 3), activation = 'relu', name = 'conv2d_3'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2), name = 'maxpool2d_3'))\n",
    "classifier.add(Conv2D(32, (3, 3), activation = 'relu', name = 'conv2d_4'))\n",
    "classifier.add(Conv2D(32, (3, 3), activation = 'relu', name = 'conv2d_5'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2), name = 'maxpool2d_5'))\n",
    "classifier.add(Flatten(name = 'flatten_5'))\n",
    "classifier.add(Dense(units = 128, activation = 'relu', name = 'dense_6'))\n",
    "classifier.add(Dropout(rate = 0.3, name = 'dropout_6'))\n",
    "classifier.add(Dense(units = 128, activation = 'relu', name = 'dense_7'))\n",
    "classifier.add(Dropout(rate = 0.3, name = 'dropout_7'))\n",
    "classifier.add(Dense(units = 10, activation = 'softmax', name = 'dense_8'))\n",
    "\n",
    "# Compile\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fit\n",
    "classifier.fit(X_train_image, y_train, batch_size = 128, epochs = 50)\n",
    "\n",
    "# Accuracy on the training subset\n",
    "print('training accuracy', classifier.evaluate(X_train_image, y_train)[1])\n",
    "\n",
    "# Accuracy on the test set\n",
    "print('test accuracy', classifier.evaluate(X_test_image, y_test)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 22, 22, 32)        9248      \n",
      "_________________________________________________________________\n",
      "maxpool2d_3 (MaxPooling2D)   (None, 11, 11, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 9, 9, 32)          9248      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 7, 32)          9248      \n",
      "_________________________________________________________________\n",
      "maxpool2d_5 (MaxPooling2D)   (None, 3, 3, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 288)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               36992     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 92,106\n",
      "Trainable params: 92,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summary 8-Layer ConvNet\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "55000/55000 [==============================] - 35s 636us/step - loss: 0.1377 - acc: 0.9573\n",
      "Epoch 2/20\n",
      "55000/55000 [==============================] - 33s 597us/step - loss: 0.0377 - acc: 0.9883\n",
      "Epoch 3/20\n",
      "55000/55000 [==============================] - 32s 586us/step - loss: 0.0263 - acc: 0.9917\n",
      "Epoch 4/20\n",
      "55000/55000 [==============================] - 32s 587us/step - loss: 0.0219 - acc: 0.9935\n",
      "Epoch 5/20\n",
      "55000/55000 [==============================] - 32s 584us/step - loss: 0.0173 - acc: 0.9944\n",
      "Epoch 6/20\n",
      "55000/55000 [==============================] - 32s 586us/step - loss: 0.0140 - acc: 0.9956\n",
      "Epoch 7/20\n",
      "55000/55000 [==============================] - 32s 588us/step - loss: 0.0126 - acc: 0.9961\n",
      "Epoch 8/20\n",
      "55000/55000 [==============================] - 32s 587us/step - loss: 0.0115 - acc: 0.9965\n",
      "Epoch 9/20\n",
      "55000/55000 [==============================] - 32s 585us/step - loss: 0.0088 - acc: 0.9973\n",
      "Epoch 10/20\n",
      "55000/55000 [==============================] - 32s 590us/step - loss: 0.0098 - acc: 0.9968\n",
      "Epoch 11/20\n",
      "55000/55000 [==============================] - 33s 594us/step - loss: 0.0065 - acc: 0.9981\n",
      "Epoch 12/20\n",
      "55000/55000 [==============================] - 32s 589us/step - loss: 0.0074 - acc: 0.9975\n",
      "Epoch 13/20\n",
      "55000/55000 [==============================] - 33s 591us/step - loss: 0.0066 - acc: 0.9982\n",
      "Epoch 14/20\n",
      "55000/55000 [==============================] - 33s 593us/step - loss: 0.0062 - acc: 0.9979\n",
      "Epoch 15/20\n",
      "55000/55000 [==============================] - 32s 589us/step - loss: 0.0054 - acc: 0.9983\n",
      "Epoch 16/20\n",
      "55000/55000 [==============================] - 32s 589us/step - loss: 0.0053 - acc: 0.9983\n",
      "Epoch 17/20\n",
      "55000/55000 [==============================] - 33s 594us/step - loss: 0.0061 - acc: 0.9983\n",
      "Epoch 18/20\n",
      "55000/55000 [==============================] - 33s 593us/step - loss: 0.0047 - acc: 0.9986\n",
      "Epoch 19/20\n",
      "55000/55000 [==============================] - 33s 598us/step - loss: 0.0049 - acc: 0.9984\n",
      "Epoch 20/20\n",
      "55000/55000 [==============================] - 33s 593us/step - loss: 0.0063 - acc: 0.9979\n",
      "55000/55000 [==============================] - 15s 281us/step\n",
      "training accuracy 0.9990363636363636\n",
      "10000/10000 [==============================] - 3s 278us/step\n",
      "test accuracy 0.9935\n"
     ]
    }
   ],
   "source": [
    "# Inception Network\n",
    "\"\"\"\n",
    "An inception network lets the network choose which filters to use when classifying an image. It is \n",
    "composed of many inception blocks where an inception block consists of many different convolutions \n",
    "with different filters concatenated into one. One would expect the inception network to have an\n",
    "absurd amount of parameters but the inception network utilizes 1 by 1 convolutions to decrease the\n",
    "amount of parameters by a considerable amount. The network below consists of 2 convolutional\n",
    "layers followed by 3 inception blocks. The output of the last inception block is then flattened\n",
    "and fed into a softmax layer.\n",
    "\"\"\"\n",
    "\n",
    "# Define the Inception block\n",
    "def inception_block(input, layer):\n",
    "    branch_1 = Conv2D(64, (1, 1), activation = 'relu', padding = 'same', \n",
    "                      name = 'conv2d_b1_'+str(layer))(input)\n",
    "    branch_2 = Conv2D(64, (1, 1), activation = 'relu', padding = 'same', \n",
    "                      name = 'conv2d_b2_'+str(layer))(input)\n",
    "    branch_2 = Conv2D(128, (3, 3), activation = 'relu', padding = 'same', \n",
    "                      name = 'conv2d_b2_'+str(layer+1))(branch_2)\n",
    "    branch_3 = Conv2D(16, (1, 1), activation = 'relu', padding = 'same', \n",
    "                      name = 'conv2d_b3_'+str(layer))(input)\n",
    "    branch_3 = Conv2D(32, (5, 5), activation = 'relu', padding = 'same', \n",
    "                      name = 'conv2d_b3_'+str(layer+1))(branch_3)\n",
    "    branch_4 = MaxPooling2D(pool_size = (3, 3), strides = (1, 1), padding = 'same', \n",
    "                            name = 'maxpool2d_b4_'+str(layer))(input)\n",
    "    branch_4 = Conv2D(32, (1, 1), activation = 'relu', padding = 'same', \n",
    "                      name = 'conv2d_b4_'+str(layer+1))(branch_4)\n",
    "    output = Concatenate(axis=-1, name = 'concat_'+str(layer+1))([branch_1, branch_2, branch_3, branch_4])\n",
    "    return output\n",
    "\n",
    "# Create the Inception Network\n",
    "X_input = Input((28, 28, 1), name = 'Input')\n",
    "X = Conv2D(64, (3, 3), activation = 'relu', padding = 'valid', name = 'conv2d_1')(X_input)\n",
    "X = Conv2D(128, (3, 3), activation = 'relu', padding = 'valid', name = 'conv2d_2')(X)\n",
    "X = MaxPooling2D(pool_size = (2, 2), name = 'maxpool2d_2')(X)\n",
    "X = inception_block(X, layer = 3)\n",
    "X = inception_block(X, layer = 5)\n",
    "X = inception_block(X, layer = 7)\n",
    "X = Flatten(name = 'flatten_8')(X)\n",
    "X = Dense(units = 10, activation='softmax', name = 'dense_9')(X)\n",
    "\n",
    "classifier = Model(inputs = X_input, outputs = X)\n",
    "\n",
    "# Compile\n",
    "classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit\n",
    "classifier.fit(X_train_image, y_train, batch_size = 128, epochs = 20)\n",
    "\n",
    "# Accuracy on the training subset\n",
    "print('training accuracy', classifier.evaluate(X_train_image, y_train)[1])\n",
    "\n",
    "# Accuracy on the test set\n",
    "print('test accuracy', classifier.evaluate(X_test_image, y_test)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input (InputLayer)              (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 26, 26, 64)   640         Input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 24, 24, 128)  73856       conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "maxpool2d_2 (MaxPooling2D)      (None, 12, 12, 128)  0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_b2_3 (Conv2D)            (None, 12, 12, 64)   8256        maxpool2d_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_b3_3 (Conv2D)            (None, 12, 12, 16)   2064        maxpool2d_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "maxpool2d_b4_3 (MaxPooling2D)   (None, 12, 12, 128)  0           maxpool2d_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_b1_3 (Conv2D)            (None, 12, 12, 64)   8256        maxpool2d_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_b2_4 (Conv2D)            (None, 12, 12, 128)  73856       conv2d_b2_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_b3_4 (Conv2D)            (None, 12, 12, 32)   12832       conv2d_b3_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_b4_4 (Conv2D)            (None, 12, 12, 32)   4128        maxpool2d_b4_3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concat_4 (Concatenate)          (None, 12, 12, 256)  0           conv2d_b1_3[0][0]                \n",
      "                                                                 conv2d_b2_4[0][0]                \n",
      "                                                                 conv2d_b3_4[0][0]                \n",
      "                                                                 conv2d_b4_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_b2_5 (Conv2D)            (None, 12, 12, 64)   16448       concat_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_b3_5 (Conv2D)            (None, 12, 12, 16)   4112        concat_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "maxpool2d_b4_5 (MaxPooling2D)   (None, 12, 12, 256)  0           concat_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_b1_5 (Conv2D)            (None, 12, 12, 64)   16448       concat_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_b2_6 (Conv2D)            (None, 12, 12, 128)  73856       conv2d_b2_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_b3_6 (Conv2D)            (None, 12, 12, 32)   12832       conv2d_b3_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_b4_6 (Conv2D)            (None, 12, 12, 32)   8224        maxpool2d_b4_5[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concat_6 (Concatenate)          (None, 12, 12, 256)  0           conv2d_b1_5[0][0]                \n",
      "                                                                 conv2d_b2_6[0][0]                \n",
      "                                                                 conv2d_b3_6[0][0]                \n",
      "                                                                 conv2d_b4_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_b2_7 (Conv2D)            (None, 12, 12, 64)   16448       concat_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_b3_7 (Conv2D)            (None, 12, 12, 16)   4112        concat_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "maxpool2d_b4_7 (MaxPooling2D)   (None, 12, 12, 256)  0           concat_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_b1_7 (Conv2D)            (None, 12, 12, 64)   16448       concat_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_b2_8 (Conv2D)            (None, 12, 12, 128)  73856       conv2d_b2_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_b3_8 (Conv2D)            (None, 12, 12, 32)   12832       conv2d_b3_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_b4_8 (Conv2D)            (None, 12, 12, 32)   8224        maxpool2d_b4_7[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concat_8 (Concatenate)          (None, 12, 12, 256)  0           conv2d_b1_7[0][0]                \n",
      "                                                                 conv2d_b2_8[0][0]                \n",
      "                                                                 conv2d_b3_8[0][0]                \n",
      "                                                                 conv2d_b4_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 36864)        0           concat_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 10)           368650      flatten_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 816,378\n",
      "Trainable params: 816,378\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summary Inception Network\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "55000/55000 [==============================] - 128s 2ms/step - loss: 0.5285 - acc: 0.8521\n",
      "Epoch 2/50\n",
      "55000/55000 [==============================] - 117s 2ms/step - loss: 0.1080 - acc: 0.9660\n",
      "Epoch 3/50\n",
      "55000/55000 [==============================] - 110s 2ms/step - loss: 0.0628 - acc: 0.9802\n",
      "Epoch 4/50\n",
      "55000/55000 [==============================] - 112s 2ms/step - loss: 0.0460 - acc: 0.9852\n",
      "Epoch 5/50\n",
      "55000/55000 [==============================] - 111s 2ms/step - loss: 0.0367 - acc: 0.9882\n",
      "Epoch 6/50\n",
      "55000/55000 [==============================] - 110s 2ms/step - loss: 0.0335 - acc: 0.9895\n",
      "Epoch 7/50\n",
      "55000/55000 [==============================] - 118s 2ms/step - loss: 0.0531 - acc: 0.9831\n",
      "Epoch 8/50\n",
      "55000/55000 [==============================] - 116s 2ms/step - loss: 0.0982 - acc: 0.9703\n",
      "Epoch 9/50\n",
      "55000/55000 [==============================] - 115s 2ms/step - loss: 0.0406 - acc: 0.9879\n",
      "Epoch 10/50\n",
      "55000/55000 [==============================] - 116s 2ms/step - loss: 0.0287 - acc: 0.9913\n",
      "Epoch 11/50\n",
      "55000/55000 [==============================] - 115s 2ms/step - loss: 0.0253 - acc: 0.9921\n",
      "Epoch 12/50\n",
      "55000/55000 [==============================] - 116s 2ms/step - loss: 0.0279 - acc: 0.9920\n",
      "Epoch 13/50\n",
      "55000/55000 [==============================] - 113s 2ms/step - loss: 0.0239 - acc: 0.9928\n",
      "Epoch 14/50\n",
      "55000/55000 [==============================] - 111s 2ms/step - loss: 0.0244 - acc: 0.9923\n",
      "Epoch 15/50\n",
      "55000/55000 [==============================] - 112s 2ms/step - loss: 0.0231 - acc: 0.9931\n",
      "Epoch 16/50\n",
      "55000/55000 [==============================] - 115s 2ms/step - loss: 0.0231 - acc: 0.9928\n",
      "Epoch 17/50\n",
      "55000/55000 [==============================] - 118s 2ms/step - loss: 0.0254 - acc: 0.9926\n",
      "Epoch 18/50\n",
      "55000/55000 [==============================] - 112s 2ms/step - loss: 0.0233 - acc: 0.9932\n",
      "Epoch 19/50\n",
      "55000/55000 [==============================] - 109s 2ms/step - loss: 0.0786 - acc: 0.9793\n",
      "Epoch 20/50\n",
      "55000/55000 [==============================] - 109s 2ms/step - loss: 0.0469 - acc: 0.9864\n",
      "Epoch 21/50\n",
      "55000/55000 [==============================] - 110s 2ms/step - loss: 0.0252 - acc: 0.9927\n",
      "Epoch 22/50\n",
      "55000/55000 [==============================] - 109s 2ms/step - loss: 0.0229 - acc: 0.9928\n",
      "Epoch 23/50\n",
      "55000/55000 [==============================] - 109s 2ms/step - loss: 0.0203 - acc: 0.9938\n",
      "Epoch 24/50\n",
      "55000/55000 [==============================] - 109s 2ms/step - loss: 0.0206 - acc: 0.9939\n",
      "Epoch 25/50\n",
      "55000/55000 [==============================] - 109s 2ms/step - loss: 0.0191 - acc: 0.9941\n",
      "Epoch 26/50\n",
      "55000/55000 [==============================] - 109s 2ms/step - loss: 0.0165 - acc: 0.9949\n",
      "Epoch 27/50\n",
      "55000/55000 [==============================] - 109s 2ms/step - loss: 0.0190 - acc: 0.9944\n",
      "Epoch 28/50\n",
      "55000/55000 [==============================] - 109s 2ms/step - loss: 0.0163 - acc: 0.9949\n",
      "Epoch 29/50\n",
      "55000/55000 [==============================] - 109s 2ms/step - loss: 0.0145 - acc: 0.9958\n",
      "Epoch 30/50\n",
      "55000/55000 [==============================] - 109s 2ms/step - loss: 0.0154 - acc: 0.9957\n",
      "Epoch 31/50\n",
      "55000/55000 [==============================] - 109s 2ms/step - loss: 0.0172 - acc: 0.9953\n",
      "Epoch 32/50\n",
      "55000/55000 [==============================] - 109s 2ms/step - loss: 0.0147 - acc: 0.9958\n",
      "Epoch 33/50\n",
      "55000/55000 [==============================] - 109s 2ms/step - loss: 0.0117 - acc: 0.9967\n",
      "Epoch 34/50\n",
      "55000/55000 [==============================] - 109s 2ms/step - loss: 0.0131 - acc: 0.9963\n",
      "Epoch 35/50\n",
      "55000/55000 [==============================] - 109s 2ms/step - loss: 0.0148 - acc: 0.9956\n",
      "Epoch 36/50\n",
      "55000/55000 [==============================] - 109s 2ms/step - loss: 0.0102 - acc: 0.9969\n",
      "Epoch 37/50\n",
      "55000/55000 [==============================] - 109s 2ms/step - loss: 0.0432 - acc: 0.9880\n",
      "Epoch 38/50\n",
      "55000/55000 [==============================] - 109s 2ms/step - loss: 0.0198 - acc: 0.9937\n",
      "Epoch 39/50\n",
      "55000/55000 [==============================] - 109s 2ms/step - loss: 0.0076 - acc: 0.9973\n",
      "Epoch 40/50\n",
      "55000/55000 [==============================] - 102s 2ms/step - loss: 0.0070 - acc: 0.9977\n",
      "Epoch 41/50\n",
      "55000/55000 [==============================] - 109s 2ms/step - loss: 0.0078 - acc: 0.9977\n",
      "Epoch 42/50\n",
      "55000/55000 [==============================] - 109s 2ms/step - loss: 0.0079 - acc: 0.9976\n",
      "Epoch 43/50\n",
      "55000/55000 [==============================] - 109s 2ms/step - loss: 0.0064 - acc: 0.9981\n",
      "Epoch 44/50\n",
      "55000/55000 [==============================] - 109s 2ms/step - loss: 0.0066 - acc: 0.9982\n",
      "Epoch 45/50\n",
      "55000/55000 [==============================] - 109s 2ms/step - loss: 0.0074 - acc: 0.9979\n",
      "Epoch 46/50\n",
      "55000/55000 [==============================] - 109s 2ms/step - loss: 0.0072 - acc: 0.9979\n",
      "Epoch 47/50\n",
      "55000/55000 [==============================] - 109s 2ms/step - loss: 0.0070 - acc: 0.9978\n",
      "Epoch 48/50\n",
      "55000/55000 [==============================] - 109s 2ms/step - loss: 0.0084 - acc: 0.9974\n",
      "Epoch 49/50\n",
      "55000/55000 [==============================] - 109s 2ms/step - loss: 0.0064 - acc: 0.9981\n",
      "Epoch 50/50\n",
      "55000/55000 [==============================] - 109s 2ms/step - loss: 0.0077 - acc: 0.9976\n",
      "55000/55000 [==============================] - 58s 1ms/step\n",
      "training accuracy 0.9952727272727273\n",
      "10000/10000 [==============================] - 10s 1ms/step\n",
      "test accuracy 0.9905\n"
     ]
    }
   ],
   "source": [
    "# Residual Network\n",
    "\"\"\"\n",
    "Experiments have shown that very deep networks degrade. The intuition is that performance worsens \n",
    "because it is difficult for a deep network to learn an identity mapping between layers. Residual\n",
    "networks (ResNets) combined with the use of batch normalization helps resolve this problem. Using a \n",
    "ResNet, we are able to train networks with over 100 layers without a drop in performance. This is \n",
    "because ResNets are able to easily learn an identity mapping between consecutive layers. A ResNet \n",
    "is made up of a series of residual blocks. These blocks contain a 'skip connection' that allows \n",
    "the gradient to be directly backpropagated to earlier layers. There are 2 main types of residual \n",
    "blocks; the identity block (input has same dimension as output) and the convolutional block (input \n",
    "has a different dimension from output). A ResNet is made up of a series of many of these different \n",
    "blocks. This particular Resnet consists of only 65 layers, however, they are more commonly used for\n",
    "much deeper networks. Note that batch normalization acts as a regulator.\n",
    "\"\"\"\n",
    "\n",
    "# Define the identity block function (output dimension = input dimension)\n",
    "def identity_block(input, filters, kernel_size, layer):\n",
    "    F1, F2, F3 = filters \n",
    "    output = Conv2D(F1, (1, 1), padding = 'valid', name = 'conv2d_'+str(layer))(input)\n",
    "    output = BatchNormalization(axis = -1, name = 'batchnorm_'+str(layer))(output)\n",
    "    output = Activation('relu', name = 'relu_'+str(layer))(output)\n",
    "    output = Conv2D(F2, (kernel_size, kernel_size), padding = 'same', \n",
    "                    name = 'conv2d_'+str(layer+1))(output)\n",
    "    output = BatchNormalization(axis = -1, name = 'batchnorm_'+str(layer+1))(output)\n",
    "    output = Activation('relu', name = 'relu_'+str(layer+1))(output)\n",
    "    output = Conv2D(F3, (1, 1), padding = 'valid', name = 'conv2d_'+str(layer+2))(output)\n",
    "    output = BatchNormalization(axis = -1, name = 'batchnorm_'+str(layer+2))(output)\n",
    "    output = Add(name = 'add_'+str(layer+2))([output, input])\n",
    "    output = Activation('relu', name = 'relu_'+str(layer+2))(output)\n",
    "    \n",
    "    return output\n",
    "    \n",
    "# Define the convolutional block function (output dimension != input dimension)\n",
    "def convolutional_block(input, filters, kernel_size, strides, layer):\n",
    "    F1, F2, F3 = filters\n",
    "    output = Conv2D(F1, (1, 1), strides = (strides, strides), padding = 'valid', \n",
    "                    name = 'conv2d_'+str(layer))(input)\n",
    "    output = BatchNormalization(axis = -1, name = 'batchnorm_'+str(layer))(output)\n",
    "    output = Activation('relu', name = 'relu_'+str(layer))(output)\n",
    "    output = Conv2D(F2, (kernel_size, kernel_size), padding = 'same', \n",
    "                    name = 'conv2d_'+str(layer+1))(output)\n",
    "    output = BatchNormalization(axis = -1, name = 'batchnorm_'+str(layer+1))(output)\n",
    "    output = Activation('relu', name = 'relu_'+str(layer+1))(output)\n",
    "    output = Conv2D(F3, (1, 1), padding = 'valid', name = 'conv2d_'+str(layer+2))(output)\n",
    "    output = BatchNormalization(axis = -1, name = 'batchnorm_'+str(layer+2))(output)\n",
    "    output_sc = Conv2D(F3, (1, 1), strides = (strides, strides), padding = 'valid', \n",
    "                       name = 'conv2d_sc_'+str(layer))(input)\n",
    "    output_sc = BatchNormalization(axis = -1, name = 'batchnorm_sc_'+str(layer))(output_sc)\n",
    "    output = Add(name = 'add_'+str(layer+2))([output, output_sc])\n",
    "    output = Activation('relu', name = 'relu_'+str(layer+2))(output)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Create the Residual Network\n",
    "X_input = Input((28, 28, 1), name = 'Input')\n",
    "X = ZeroPadding2D((2, 2), name = 'zeropad2d')(X_input)\n",
    "X = Conv2D(32, (3, 3), padding = 'same', strides = (2,2), name = 'conv2d_1')(X)\n",
    "X = BatchNormalization(axis = -1, name = 'batchnorm_1')(X)\n",
    "X = Activation('relu', name = 'relu_1')(X)\n",
    "X = convolutional_block(X, filters = [32, 32, 128], kernel_size = 3, strides = 1, layer = 2)\n",
    "X = identity_block(X, filters = [32, 32, 128], kernel_size = 3, layer = 5)\n",
    "X = identity_block(X, filters = [32, 32, 128], kernel_size = 3, layer = 8)\n",
    "X = convolutional_block(X, filters = [64, 64, 256], kernel_size = 3, strides = 2, layer = 11)\n",
    "X = identity_block(X, filters = [64, 64, 256], kernel_size = 3, layer = 14)\n",
    "X = identity_block(X, filters = [64, 64, 256], kernel_size = 3, layer = 17)\n",
    "X = identity_block(X, filters = [64, 64, 256], kernel_size = 3, layer = 20)\n",
    "X = convolutional_block(X, filters = [128, 128, 512], kernel_size = 3, strides = 2, layer = 23)\n",
    "X = identity_block(X, filters = [128, 128, 512], kernel_size = 3, layer = 26)\n",
    "X = identity_block(X, filters = [128, 128, 512], kernel_size = 3, layer = 29)\n",
    "X = identity_block(X, filters = [128, 128, 512], kernel_size = 3, layer = 32)\n",
    "X = identity_block(X, filters = [128, 128, 512], kernel_size = 3, layer = 35)\n",
    "X = convolutional_block(X, filters = [256, 256, 1024], kernel_size = 3, strides = 2, layer = 38)\n",
    "X = identity_block(X, filters = [256, 256, 1024], kernel_size = 3, layer = 41)\n",
    "X = identity_block(X, filters = [256, 256, 1024], kernel_size = 3, layer = 44)\n",
    "X = identity_block(X, filters = [256, 256, 1024], kernel_size = 3, layer = 47)\n",
    "X = identity_block(X, filters = [256, 256, 1024], kernel_size = 3, layer = 50)\n",
    "X = identity_block(X, filters = [256, 256, 1024], kernel_size = 3, layer = 53)\n",
    "X = convolutional_block(X, filters = [512, 512, 2048], kernel_size = 3, strides = 2, layer = 56)\n",
    "X = identity_block(X, filters = [512, 512, 2048], kernel_size = 3, layer = 59)\n",
    "X = identity_block(X, filters = [512, 512, 2048], kernel_size = 3, layer = 62)\n",
    "X = Flatten(name = 'flatten_64')(X)\n",
    "X = Dense(units=10, kernel_initializer = 'glorot_uniform', activation='softmax', name = 'dense_65')(X)\n",
    "\n",
    "classifier = Model(inputs = X_input, outputs = X)\n",
    "\n",
    "# Compile\n",
    "classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit\n",
    "classifier.fit(X_train_image, y_train, batch_size = 128, epochs = 50)\n",
    "\n",
    "# Accuracy on the training subset\n",
    "print('training accuracy', classifier.evaluate(X_train_image, y_train)[1])\n",
    "\n",
    "# Accuracy on the test set\n",
    "print('test accuracy', classifier.evaluate(X_test_image, y_test)[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input (InputLayer)              (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zeropad2d (ZeroPadding2D)       (None, 32, 32, 1)    0           Input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 16, 16, 32)   320         zeropad2d[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_1 (BatchNormalization (None, 16, 16, 32)   128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "relu_1 (Activation)             (None, 16, 16, 32)   0           batchnorm_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 16, 16, 32)   1056        relu_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_2 (BatchNormalization (None, 16, 16, 32)   128         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "relu_2 (Activation)             (None, 16, 16, 32)   0           batchnorm_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 16, 16, 32)   9248        relu_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_3 (BatchNormalization (None, 16, 16, 32)   128         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "relu_3 (Activation)             (None, 16, 16, 32)   0           batchnorm_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 16, 16, 128)  4224        relu_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_sc_2 (Conv2D)            (None, 16, 16, 128)  4224        relu_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_4 (BatchNormalization (None, 16, 16, 128)  512         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_sc_2 (BatchNormalizat (None, 16, 16, 128)  512         conv2d_sc_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 128)  0           batchnorm_4[0][0]                \n",
      "                                                                 batchnorm_sc_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "relu_4 (Activation)             (None, 16, 16, 128)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 16, 16, 32)   4128        relu_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_5 (BatchNormalization (None, 16, 16, 32)   128         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "relu_5 (Activation)             (None, 16, 16, 32)   0           batchnorm_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 16, 32)   9248        relu_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_6 (BatchNormalization (None, 16, 16, 32)   128         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "relu_6 (Activation)             (None, 16, 16, 32)   0           batchnorm_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 128)  4224        relu_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_7 (BatchNormalization (None, 16, 16, 128)  512         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 16, 16, 128)  0           batchnorm_7[0][0]                \n",
      "                                                                 relu_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu_7 (Activation)             (None, 16, 16, 128)  0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 32)   4128        relu_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_8 (BatchNormalization (None, 16, 16, 32)   128         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "relu_8 (Activation)             (None, 16, 16, 32)   0           batchnorm_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 32)   9248        relu_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_9 (BatchNormalization (None, 16, 16, 32)   128         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "relu_9 (Activation)             (None, 16, 16, 32)   0           batchnorm_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 128)  4224        relu_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_10 (BatchNormalizatio (None, 16, 16, 128)  512         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 16, 16, 128)  0           batchnorm_10[0][0]               \n",
      "                                                                 relu_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu_10 (Activation)            (None, 16, 16, 128)  0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 8, 8, 64)     8256        relu_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_11 (BatchNormalizatio (None, 8, 8, 64)     256         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "relu_11 (Activation)            (None, 8, 8, 64)     0           batchnorm_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 8, 8, 64)     36928       relu_11[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_12 (BatchNormalizatio (None, 8, 8, 64)     256         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "relu_12 (Activation)            (None, 8, 8, 64)     0           batchnorm_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 8, 8, 256)    16640       relu_12[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_sc_11 (Conv2D)           (None, 8, 8, 256)    33024       relu_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_13 (BatchNormalizatio (None, 8, 8, 256)    1024        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_sc_11 (BatchNormaliza (None, 8, 8, 256)    1024        conv2d_sc_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 8, 8, 256)    0           batchnorm_13[0][0]               \n",
      "                                                                 batchnorm_sc_11[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "relu_13 (Activation)            (None, 8, 8, 256)    0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 8, 8, 64)     16448       relu_13[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_14 (BatchNormalizatio (None, 8, 8, 64)     256         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "relu_14 (Activation)            (None, 8, 8, 64)     0           batchnorm_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 64)     36928       relu_14[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_15 (BatchNormalizatio (None, 8, 8, 64)     256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "relu_15 (Activation)            (None, 8, 8, 64)     0           batchnorm_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 256)    16640       relu_15[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_16 (BatchNormalizatio (None, 8, 8, 256)    1024        conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 8, 8, 256)    0           batchnorm_16[0][0]               \n",
      "                                                                 relu_13[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "relu_16 (Activation)            (None, 8, 8, 256)    0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 8, 8, 64)     16448       relu_16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_17 (BatchNormalizatio (None, 8, 8, 64)     256         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "relu_17 (Activation)            (None, 8, 8, 64)     0           batchnorm_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 8, 8, 64)     36928       relu_17[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_18 (BatchNormalizatio (None, 8, 8, 64)     256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "relu_18 (Activation)            (None, 8, 8, 64)     0           batchnorm_18[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 8, 8, 256)    16640       relu_18[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_19 (BatchNormalizatio (None, 8, 8, 256)    1024        conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 8, 8, 256)    0           batchnorm_19[0][0]               \n",
      "                                                                 relu_16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "relu_19 (Activation)            (None, 8, 8, 256)    0           add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 8, 8, 64)     16448       relu_19[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_20 (BatchNormalizatio (None, 8, 8, 64)     256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "relu_20 (Activation)            (None, 8, 8, 64)     0           batchnorm_20[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 8, 8, 64)     36928       relu_20[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_21 (BatchNormalizatio (None, 8, 8, 64)     256         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "relu_21 (Activation)            (None, 8, 8, 64)     0           batchnorm_21[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 8, 8, 256)    16640       relu_21[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_22 (BatchNormalizatio (None, 8, 8, 256)    1024        conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 8, 8, 256)    0           batchnorm_22[0][0]               \n",
      "                                                                 relu_19[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "relu_22 (Activation)            (None, 8, 8, 256)    0           add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 4, 4, 128)    32896       relu_22[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_23 (BatchNormalizatio (None, 4, 4, 128)    512         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "relu_23 (Activation)            (None, 4, 4, 128)    0           batchnorm_23[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 4, 4, 128)    147584      relu_23[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_24 (BatchNormalizatio (None, 4, 4, 128)    512         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "relu_24 (Activation)            (None, 4, 4, 128)    0           batchnorm_24[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 4, 4, 512)    66048       relu_24[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_sc_23 (Conv2D)           (None, 4, 4, 512)    131584      relu_22[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_25 (BatchNormalizatio (None, 4, 4, 512)    2048        conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_sc_23 (BatchNormaliza (None, 4, 4, 512)    2048        conv2d_sc_23[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 4, 4, 512)    0           batchnorm_25[0][0]               \n",
      "                                                                 batchnorm_sc_23[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "relu_25 (Activation)            (None, 4, 4, 512)    0           add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 4, 4, 128)    65664       relu_25[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_26 (BatchNormalizatio (None, 4, 4, 128)    512         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "relu_26 (Activation)            (None, 4, 4, 128)    0           batchnorm_26[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 4, 4, 128)    147584      relu_26[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_27 (BatchNormalizatio (None, 4, 4, 128)    512         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "relu_27 (Activation)            (None, 4, 4, 128)    0           batchnorm_27[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 4, 4, 512)    66048       relu_27[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_28 (BatchNormalizatio (None, 4, 4, 512)    2048        conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 4, 4, 512)    0           batchnorm_28[0][0]               \n",
      "                                                                 relu_25[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "relu_28 (Activation)            (None, 4, 4, 512)    0           add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 4, 4, 128)    65664       relu_28[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_29 (BatchNormalizatio (None, 4, 4, 128)    512         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "relu_29 (Activation)            (None, 4, 4, 128)    0           batchnorm_29[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 4, 4, 128)    147584      relu_29[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_30 (BatchNormalizatio (None, 4, 4, 128)    512         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "relu_30 (Activation)            (None, 4, 4, 128)    0           batchnorm_30[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 4, 4, 512)    66048       relu_30[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_31 (BatchNormalizatio (None, 4, 4, 512)    2048        conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 4, 4, 512)    0           batchnorm_31[0][0]               \n",
      "                                                                 relu_28[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "relu_31 (Activation)            (None, 4, 4, 512)    0           add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 4, 4, 128)    65664       relu_31[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_32 (BatchNormalizatio (None, 4, 4, 128)    512         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "relu_32 (Activation)            (None, 4, 4, 128)    0           batchnorm_32[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 4, 4, 128)    147584      relu_32[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_33 (BatchNormalizatio (None, 4, 4, 128)    512         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "relu_33 (Activation)            (None, 4, 4, 128)    0           batchnorm_33[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 4, 4, 512)    66048       relu_33[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_34 (BatchNormalizatio (None, 4, 4, 512)    2048        conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_34 (Add)                    (None, 4, 4, 512)    0           batchnorm_34[0][0]               \n",
      "                                                                 relu_31[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "relu_34 (Activation)            (None, 4, 4, 512)    0           add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 4, 4, 128)    65664       relu_34[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_35 (BatchNormalizatio (None, 4, 4, 128)    512         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "relu_35 (Activation)            (None, 4, 4, 128)    0           batchnorm_35[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 4, 4, 128)    147584      relu_35[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_36 (BatchNormalizatio (None, 4, 4, 128)    512         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "relu_36 (Activation)            (None, 4, 4, 128)    0           batchnorm_36[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 4, 4, 512)    66048       relu_36[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_37 (BatchNormalizatio (None, 4, 4, 512)    2048        conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_37 (Add)                    (None, 4, 4, 512)    0           batchnorm_37[0][0]               \n",
      "                                                                 relu_34[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "relu_37 (Activation)            (None, 4, 4, 512)    0           add_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 2, 2, 256)    131328      relu_37[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_38 (BatchNormalizatio (None, 2, 2, 256)    1024        conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "relu_38 (Activation)            (None, 2, 2, 256)    0           batchnorm_38[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 2, 2, 256)    590080      relu_38[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_39 (BatchNormalizatio (None, 2, 2, 256)    1024        conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "relu_39 (Activation)            (None, 2, 2, 256)    0           batchnorm_39[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 2, 2, 1024)   263168      relu_39[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_sc_38 (Conv2D)           (None, 2, 2, 1024)   525312      relu_37[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_40 (BatchNormalizatio (None, 2, 2, 1024)   4096        conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_sc_38 (BatchNormaliza (None, 2, 2, 1024)   4096        conv2d_sc_38[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_40 (Add)                    (None, 2, 2, 1024)   0           batchnorm_40[0][0]               \n",
      "                                                                 batchnorm_sc_38[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "relu_40 (Activation)            (None, 2, 2, 1024)   0           add_40[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 2, 2, 256)    262400      relu_40[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_41 (BatchNormalizatio (None, 2, 2, 256)    1024        conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "relu_41 (Activation)            (None, 2, 2, 256)    0           batchnorm_41[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 2, 2, 256)    590080      relu_41[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_42 (BatchNormalizatio (None, 2, 2, 256)    1024        conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "relu_42 (Activation)            (None, 2, 2, 256)    0           batchnorm_42[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 2, 2, 1024)   263168      relu_42[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_43 (BatchNormalizatio (None, 2, 2, 1024)   4096        conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_43 (Add)                    (None, 2, 2, 1024)   0           batchnorm_43[0][0]               \n",
      "                                                                 relu_40[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "relu_43 (Activation)            (None, 2, 2, 1024)   0           add_43[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 2, 2, 256)    262400      relu_43[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_44 (BatchNormalizatio (None, 2, 2, 256)    1024        conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "relu_44 (Activation)            (None, 2, 2, 256)    0           batchnorm_44[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 2, 2, 256)    590080      relu_44[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_45 (BatchNormalizatio (None, 2, 2, 256)    1024        conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "relu_45 (Activation)            (None, 2, 2, 256)    0           batchnorm_45[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 2, 2, 1024)   263168      relu_45[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_46 (BatchNormalizatio (None, 2, 2, 1024)   4096        conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_46 (Add)                    (None, 2, 2, 1024)   0           batchnorm_46[0][0]               \n",
      "                                                                 relu_43[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "relu_46 (Activation)            (None, 2, 2, 1024)   0           add_46[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 2, 2, 256)    262400      relu_46[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_47 (BatchNormalizatio (None, 2, 2, 256)    1024        conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "relu_47 (Activation)            (None, 2, 2, 256)    0           batchnorm_47[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 2, 2, 256)    590080      relu_47[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_48 (BatchNormalizatio (None, 2, 2, 256)    1024        conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "relu_48 (Activation)            (None, 2, 2, 256)    0           batchnorm_48[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 2, 2, 1024)   263168      relu_48[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_49 (BatchNormalizatio (None, 2, 2, 1024)   4096        conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_49 (Add)                    (None, 2, 2, 1024)   0           batchnorm_49[0][0]               \n",
      "                                                                 relu_46[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "relu_49 (Activation)            (None, 2, 2, 1024)   0           add_49[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 2, 2, 256)    262400      relu_49[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_50 (BatchNormalizatio (None, 2, 2, 256)    1024        conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "relu_50 (Activation)            (None, 2, 2, 256)    0           batchnorm_50[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 2, 2, 256)    590080      relu_50[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_51 (BatchNormalizatio (None, 2, 2, 256)    1024        conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "relu_51 (Activation)            (None, 2, 2, 256)    0           batchnorm_51[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 2, 2, 1024)   263168      relu_51[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_52 (BatchNormalizatio (None, 2, 2, 1024)   4096        conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_52 (Add)                    (None, 2, 2, 1024)   0           batchnorm_52[0][0]               \n",
      "                                                                 relu_49[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "relu_52 (Activation)            (None, 2, 2, 1024)   0           add_52[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 2, 2, 256)    262400      relu_52[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_53 (BatchNormalizatio (None, 2, 2, 256)    1024        conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "relu_53 (Activation)            (None, 2, 2, 256)    0           batchnorm_53[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 2, 2, 256)    590080      relu_53[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_54 (BatchNormalizatio (None, 2, 2, 256)    1024        conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "relu_54 (Activation)            (None, 2, 2, 256)    0           batchnorm_54[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 2, 2, 1024)   263168      relu_54[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_55 (BatchNormalizatio (None, 2, 2, 1024)   4096        conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_55 (Add)                    (None, 2, 2, 1024)   0           batchnorm_55[0][0]               \n",
      "                                                                 relu_52[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "relu_55 (Activation)            (None, 2, 2, 1024)   0           add_55[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 1, 1, 512)    524800      relu_55[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_56 (BatchNormalizatio (None, 1, 1, 512)    2048        conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "relu_56 (Activation)            (None, 1, 1, 512)    0           batchnorm_56[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 1, 1, 512)    2359808     relu_56[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_57 (BatchNormalizatio (None, 1, 1, 512)    2048        conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "relu_57 (Activation)            (None, 1, 1, 512)    0           batchnorm_57[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 1, 1, 2048)   1050624     relu_57[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_sc_56 (Conv2D)           (None, 1, 1, 2048)   2099200     relu_55[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_58 (BatchNormalizatio (None, 1, 1, 2048)   8192        conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_sc_56 (BatchNormaliza (None, 1, 1, 2048)   8192        conv2d_sc_56[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_58 (Add)                    (None, 1, 1, 2048)   0           batchnorm_58[0][0]               \n",
      "                                                                 batchnorm_sc_56[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "relu_58 (Activation)            (None, 1, 1, 2048)   0           add_58[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 1, 1, 512)    1049088     relu_58[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_59 (BatchNormalizatio (None, 1, 1, 512)    2048        conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "relu_59 (Activation)            (None, 1, 1, 512)    0           batchnorm_59[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 1, 1, 512)    2359808     relu_59[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_60 (BatchNormalizatio (None, 1, 1, 512)    2048        conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "relu_60 (Activation)            (None, 1, 1, 512)    0           batchnorm_60[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 1, 1, 2048)   1050624     relu_60[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_61 (BatchNormalizatio (None, 1, 1, 2048)   8192        conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_61 (Add)                    (None, 1, 1, 2048)   0           batchnorm_61[0][0]               \n",
      "                                                                 relu_58[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "relu_61 (Activation)            (None, 1, 1, 2048)   0           add_61[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 1, 1, 512)    1049088     relu_61[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_62 (BatchNormalizatio (None, 1, 1, 512)    2048        conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "relu_62 (Activation)            (None, 1, 1, 512)    0           batchnorm_62[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 1, 1, 512)    2359808     relu_62[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_63 (BatchNormalizatio (None, 1, 1, 512)    2048        conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "relu_63 (Activation)            (None, 1, 1, 512)    0           batchnorm_63[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 1, 1, 2048)   1050624     relu_63[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_64 (BatchNormalizatio (None, 1, 1, 2048)   8192        conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_64 (Add)                    (None, 1, 1, 2048)   0           batchnorm_64[0][0]               \n",
      "                                                                 relu_61[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "relu_64 (Activation)            (None, 1, 1, 2048)   0           add_64[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_64 (Flatten)            (None, 2048)         0           relu_64[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_65 (Dense)                (None, 10)           20490       flatten_64[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 24,030,090\n",
      "Trainable params: 23,973,322\n",
      "Non-trainable params: 56,768\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summary ResNet\n",
    "classifier.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
