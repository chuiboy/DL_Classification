{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Deep Learning: Classification with TensorFlow\n",
    "___\n",
    "\n",
    "#### Summary:\n",
    "\n",
    "Deep Learning has grown in popularity recently due to better GPUs and the large amount of data we have access to. Combining the large amount of data with better GPUs, we are able to train deep neural networks in a reasonable amount of time that outperform traditional machine learning models such as Logistic/Softmax Regression, SVM, Naive Bayes, K-Nearest Neighbor, etc.\n",
    "\n",
    "When building a deep neural network, it is common to use APIs like Tensorflow and Keras. Tensorflow is an API that runs on top of Python and contains many useful low-level functions. Keras is an API that runs on top of TensorFlow and has many high-level functions but is usually more restrictive. When building a neural network, Keras will usually contain all the tools we need, however, it is useful to know how to use TensorFlow and so in this notebook we will strictly be using TensorFlow. Note that we will not get into hyperparameter tuning here and we will not use a validation set (The test set is used as the validaiton set here).\n",
    "___\n",
    "#### This notebook will include:\n",
    "1. Softmax Regression\n",
    "2. 3-Layer Standard Neural Network (Multilayer Perceptron)\n",
    "3. 8-Layer Standard Neural Network\n",
    "4. 3-Layer Convolutional Network\n",
    "5. 8-Layer Convolutional Network\n",
    "___\n",
    "#### Reference: \n",
    "\n",
    "Much of what is in this notebook was learned from the Deep Learning Specialization Coursera course by Andrew Ng and from the Tensorflow tutorial at https://www.tensorflow.org/get_started/mnist/pros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Datasets/MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting Datasets/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting Datasets/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting Datasets/MNIST/t10k-labels-idx1-ubyte.gz\n",
      "X_train: (55000, 784)\n",
      "X_test: (10000, 784)\n",
      "y_train: (55000, 10)\n",
      "y_test: (10000, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a5e8cf8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x5b3c240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The dataset that will be used for this notebook is the MNIST dataset which consists of hand-drawn digits \n",
    "ranging from 0 to 9. The dataset is separated into 55000 training examples, 5000 validation examples\n",
    "and 10000 test examples. Each example consists of 784 input features corresponding to the 784 pixel values \n",
    "of the 28x28 sized image. The dataset has already been preprocessed (divided by 255).\n",
    "\"\"\"\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Import the dataset (one-hot encoded)\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "dataset = input_data.read_data_sets(\"Datasets/MNIST/\", one_hot=True)\n",
    "\n",
    "X_train = dataset.train.images\n",
    "y_train = dataset.train.labels\n",
    "X_test = dataset.test.images\n",
    "y_test = dataset.test.labels\n",
    "\n",
    "# Printing the dataset shape\n",
    "print('X_train:', X_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('y_test:', y_test.shape)\n",
    "\n",
    "# Display an example from the dataset\n",
    "sample = Image.fromarray(255*X_train[np.random.randint(X_train.shape[0]), :].reshape(28,28))\n",
    "plt.imshow(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 500, training accuracy 0.89\n",
      "step 1000, training accuracy 0.88\n",
      "step 1500, training accuracy 0.9\n",
      "step 2000, training accuracy 0.94\n",
      "step 2500, training accuracy 0.93\n",
      "step 3000, training accuracy 0.93\n",
      "step 3500, training accuracy 0.96\n",
      "step 4000, training accuracy 0.92\n",
      "step 4500, training accuracy 0.88\n",
      "step 5000, training accuracy 0.95\n",
      "step 5500, training accuracy 0.93\n",
      "step 6000, training accuracy 0.94\n",
      "step 6500, training accuracy 0.91\n",
      "step 7000, training accuracy 0.97\n",
      "step 7500, training accuracy 0.96\n",
      "step 8000, training accuracy 0.95\n",
      "step 8500, training accuracy 0.93\n",
      "step 9000, training accuracy 0.9\n",
      "step 9500, training accuracy 0.96\n",
      "step 10000, training accuracy 0.9\n",
      "test accuracy 0.9265\n"
     ]
    }
   ],
   "source": [
    "# Softmax regression\n",
    "\"\"\"\n",
    "The simplest approach to a classification problem with more than 2 categories is to apply softmax\n",
    "regression. Since softmax regression is essentially a single-layer neural network, it is expected\n",
    "to perform poorly. \n",
    "\"\"\"\n",
    "\n",
    "# Placeholders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# Parameters\n",
    "initializer = tf.contrib.layers.xavier_initializer()\n",
    "W = tf.Variable(initializer([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "Z = tf.matmul(X, W) + b # Logit\n",
    "\n",
    "# Cost\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = Z))\n",
    "\n",
    "# Single training step\n",
    "train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)\n",
    "\n",
    "# Accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(Z, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Create a session\n",
    "sess = tf.InteractiveSession()\n",
    "# Initialize variables\n",
    "tf.global_variables_initializer().run()\n",
    "# Perform 10000 iterations of backward and forward propagation\n",
    "for i in range(1, 10001):\n",
    "    batch = dataset.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict = {X: batch[0], y: batch[1]})\n",
    "    if i % 500 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict = {X: batch[0], y: batch[1]})\n",
    "        print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "\n",
    "# Test accuracy    \n",
    "print('test accuracy', accuracy.eval(feed_dict = {X: dataset.test.images, y: dataset.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 500, training accuracy 0.97\n",
      "step 1000, training accuracy 0.97\n",
      "step 1500, training accuracy 0.93\n",
      "step 2000, training accuracy 0.93\n",
      "step 2500, training accuracy 0.92\n",
      "step 3000, training accuracy 0.97\n",
      "step 3500, training accuracy 0.95\n",
      "step 4000, training accuracy 0.95\n",
      "step 4500, training accuracy 0.96\n",
      "step 5000, training accuracy 0.96\n",
      "step 5500, training accuracy 0.99\n",
      "step 6000, training accuracy 0.97\n",
      "step 6500, training accuracy 0.98\n",
      "step 7000, training accuracy 0.97\n",
      "step 7500, training accuracy 0.99\n",
      "step 8000, training accuracy 0.95\n",
      "step 8500, training accuracy 0.97\n",
      "step 9000, training accuracy 0.97\n",
      "step 9500, training accuracy 0.95\n",
      "step 10000, training accuracy 0.95\n",
      "test accuracy 0.9621\n"
     ]
    }
   ],
   "source": [
    "# 3-Layer Standard Neural Network\n",
    "\"\"\"\n",
    "A 3-layer neural network, although not deep, usually has enough layers to outperform traditional \n",
    "Machine Learning algorithms. These extra layers allow the network to learn more complex features \n",
    "that can be useful for classification. \n",
    "\"\"\"\n",
    "\n",
    "# Parameter initialization functions\n",
    "def weight_variable(shape):\n",
    "    initializer = tf.contrib.layers.xavier_initializer()\n",
    "    return tf.Variable(initializer(shape))\n",
    "\n",
    "def bias_variable(shape):\n",
    "    return tf.Variable(tf.zeros(shape))\n",
    "\n",
    "# Placeholders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# First FC layer (20 hidden units)\n",
    "W_fc1 = weight_variable([784, 20])\n",
    "b_fc1 = bias_variable([20])\n",
    "A_fc1 = tf.nn.relu(tf.matmul(X, W_fc1) + b_fc1)\n",
    "\n",
    "# Second FC layer (20 hidden units)\n",
    "W_fc2 = weight_variable([20, 20])\n",
    "b_fc2 = bias_variable([20])\n",
    "A_fc2 = tf.nn.relu(tf.matmul(A_fc1, W_fc2) + b_fc2)\n",
    "\n",
    "# Third FC layer (20 hidden units)\n",
    "W_fc3 = weight_variable([20, 20])\n",
    "b_fc3 = bias_variable([20])\n",
    "A_fc3 = tf.nn.relu(tf.matmul(A_fc2, W_fc3) + b_fc3)\n",
    "\n",
    "# Output softmax layer (10 units corresponding to the labels 0-9)\n",
    "W_fc4 = weight_variable([20, 10])\n",
    "b_fc4 = bias_variable([10])\n",
    "Z_fc4 = tf.matmul(A_fc3, W_fc4) + b_fc4 # Logit\n",
    "\n",
    "# Cost\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = Z_fc4))\n",
    "\n",
    "# Single training step\n",
    "train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)\n",
    "\n",
    "# Accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(Z_fc4, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Create a session\n",
    "sess = tf.InteractiveSession()\n",
    "# Initialize variables\n",
    "tf.global_variables_initializer().run()\n",
    "# Perform 10000 iterations of backward and forward propagation\n",
    "for i in range(1, 10001):\n",
    "    batch = dataset.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict = {X: batch[0], y: batch[1]})\n",
    "    if i % 500 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict = {X: batch[0], y: batch[1]})\n",
    "        print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "\n",
    "# Test accuracy    \n",
    "print('test accuracy', accuracy.eval(feed_dict = {X: dataset.test.images, y: dataset.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 500, training accuracy 0.99\n",
      "step 1000, training accuracy 0.99\n",
      "step 1500, training accuracy 1\n",
      "step 2000, training accuracy 1\n",
      "step 2500, training accuracy 0.99\n",
      "step 3000, training accuracy 1\n",
      "step 3500, training accuracy 1\n",
      "step 4000, training accuracy 1\n",
      "step 4500, training accuracy 1\n",
      "step 5000, training accuracy 0.99\n",
      "step 5500, training accuracy 1\n",
      "step 6000, training accuracy 1\n",
      "step 6500, training accuracy 1\n",
      "step 7000, training accuracy 1\n",
      "step 7500, training accuracy 1\n",
      "step 8000, training accuracy 1\n",
      "step 8500, training accuracy 1\n",
      "step 9000, training accuracy 1\n",
      "step 9500, training accuracy 1\n",
      "step 10000, training accuracy 1\n",
      "test accuracy 0.9929\n"
     ]
    }
   ],
   "source": [
    "# 3-layer ConvNet\n",
    "\"\"\"\n",
    "A Convolutional neural network performs much better than standard neural networks when it comes to \n",
    "computer vision tasks. This is because it tries to learn features or rather feature detectors from \n",
    "batches of pixels, taking into account the shape of the image. The parameters learned come in the \n",
    "form of filters. There are also considerable fewer parameters, which result from parameter sharing \n",
    "and sparsity of connections. This particular ConvNet has 2 convolutional layer and 1 fully-connected \n",
    "layers.\n",
    "\"\"\"\n",
    "\n",
    "# Parameter initialization functions\n",
    "def weight_variable(shape):\n",
    "    initializer = tf.contrib.layers.xavier_initializer()\n",
    "    return tf.Variable(initializer(shape))\n",
    "\n",
    "def bias_variable(shape):\n",
    "    return tf.Variable(tf.zeros(shape))\n",
    "\n",
    "# Convolution and pooling functions\n",
    "def conv2d(X, W): \n",
    "    return tf.nn.conv2d(X, W, strides = [1, 1, 1, 1], padding = 'VALID')\n",
    "\n",
    "def max_pool_2x2(X):\n",
    "    return tf.nn.max_pool(X, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding = 'SAME')\n",
    "\n",
    "# Placeholders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# First convolutional layer (32 5x5x1 filters > max-pool)\n",
    "X_image = tf.reshape(X, [-1, 28, 28, 1])\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "A_conv1 = tf.nn.relu(conv2d(X_image, W_conv1) + b_conv1)\n",
    "A_pool1 = max_pool_2x2(A_conv1) \n",
    "# Output shape (m, 12, 12, 32)\n",
    "\n",
    "# Second convolutional layer (64 3x3x32 filters > max-pool)\n",
    "W_conv2 = weight_variable([3, 3, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "A_conv2 = tf.nn.relu(conv2d(A_pool1, W_conv2) + b_conv2)\n",
    "A_pool2 = max_pool_2x2(A_conv2) \n",
    "# Output shape (m, 5, 5, 64)\n",
    "\n",
    "# Fully-connected layer (1024 hidden units)\n",
    "W_fc3 = weight_variable([5*5*64, 1024])\n",
    "b_fc3 = bias_variable([1024])\n",
    "A_pool2_flat = tf.reshape(A_pool2, [-1, 5*5*64])\n",
    "A_fc3 = tf.nn.relu(tf.matmul(A_pool2_flat, W_fc3) + b_fc3) \n",
    "A_fc3_drop = tf.nn.dropout(A_fc3, keep_prob) # Dropout regularization\n",
    "# Output shape (m, 1024)\n",
    "\n",
    "# Output softmax layer (10 units corresponding to the labels 0-9)\n",
    "W_fc4 = weight_variable([1024, 10])\n",
    "b_fc4 = bias_variable([10])\n",
    "Z_fc4 = tf.matmul(A_fc3_drop, W_fc4) + b_fc4 # Logit\n",
    "\n",
    "# Cost\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = Z_fc4))\n",
    "\n",
    "# Single training step\n",
    "train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)\n",
    "\n",
    "# Accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(Z_fc4, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Create a session\n",
    "sess = tf.InteractiveSession()\n",
    "# Initialize variables\n",
    "tf.global_variables_initializer().run()\n",
    "# Perform 10000 iterations of backward and forward propagation\n",
    "for i in range(1, 10001):\n",
    "    batch = dataset.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict = {X: batch[0], y: batch[1], keep_prob: 0.5})\n",
    "    if i % 500 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict = {X: batch[0], y: batch[1], keep_prob: 1})\n",
    "        print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "\n",
    "# Test accuracy    \n",
    "print('test accuracy', \n",
    "      accuracy.eval(feed_dict = {X: dataset.test.images, y: dataset.test.labels, keep_prob: 1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 500, training accuracy 0.97\n",
      "step 1000, training accuracy 0.99\n",
      "step 1500, training accuracy 1\n",
      "step 2000, training accuracy 1\n",
      "step 2500, training accuracy 1\n",
      "step 3000, training accuracy 1\n",
      "step 3500, training accuracy 1\n",
      "step 4000, training accuracy 1\n",
      "step 4500, training accuracy 1\n",
      "step 5000, training accuracy 1\n",
      "step 5500, training accuracy 1\n",
      "step 6000, training accuracy 1\n",
      "step 6500, training accuracy 1\n",
      "step 7000, training accuracy 1\n",
      "step 7500, training accuracy 1\n",
      "step 8000, training accuracy 1\n",
      "step 8500, training accuracy 1\n",
      "step 9000, training accuracy 1\n",
      "step 9500, training accuracy 1\n",
      "step 10000, training accuracy 1\n",
      "test accuracy 0.9946000051498413\n"
     ]
    }
   ],
   "source": [
    "# Inception Network\n",
    "\"\"\"\n",
    "An inception network lets the network choose which filters to use when classifying an image. It is \n",
    "composed of many inception blocks where an inception block consists of many different convolutions \n",
    "with different filters concatenated into one. One would expect the inception network to have an\n",
    "absurd amount of parameters but the inception network utilizes 1 by 1 convolutions to decrease the\n",
    "amount of parameters by a considerable amount. The network below consists of 2 convolutional\n",
    "layers followed by 3 inception blocks. The output of the last inception block is then flattened\n",
    "and fed into a softmax layer.\n",
    "\"\"\"\n",
    "\n",
    "# Parameter initialization functions\n",
    "def weight_variable(shape):\n",
    "    initializer = tf.contrib.layers.xavier_initializer()\n",
    "    return tf.Variable(initializer(shape))\n",
    "\n",
    "def bias_variable(shape):\n",
    "    return tf.Variable(tf.zeros(shape))\n",
    "\n",
    "# Convolution function\n",
    "def conv2d(X, W): \n",
    "    return tf.nn.conv2d(X, W, strides = [1, 1, 1, 1], padding = 'VALID')\n",
    "\n",
    "# Define the Inception block function\n",
    "def inception_block(X):\n",
    "    num_channels = X.get_shape()[3].value\n",
    "    \n",
    "    # Branch 1\n",
    "    W_conv1_1 = weight_variable([1, 1, num_channels, 64])\n",
    "    b_conv1_1 = bias_variable([64])\n",
    "    A_conv1_1 = tf.nn.relu(tf.nn.conv2d(X, W_conv1_1, strides = [1, 1, 1, 1], \n",
    "                                        padding = 'VALID') + b_conv1_1)\n",
    "    \n",
    "    # Branch 2\n",
    "    W_conv2_1 = weight_variable([1, 1, num_channels, 64])\n",
    "    b_conv2_1 = bias_variable([64])\n",
    "    A_conv2_1 = tf.nn.relu(tf.nn.conv2d(X, W_conv2_1, strides = [1, 1, 1, 1], \n",
    "                                        padding = 'VALID') + b_conv2_1)\n",
    "    \n",
    "    W_conv2_2 = weight_variable([3, 3, 64, 128])\n",
    "    b_conv2_2 = bias_variable([128])\n",
    "    A_conv2_2 = tf.nn.relu(tf.nn.conv2d(A_conv2_1, W_conv2_2, strides = [1, 1, 1, 1], \n",
    "                                        padding = 'SAME') + b_conv2_2)\n",
    "    \n",
    "    # Branch 3\n",
    "    W_conv3_1 = weight_variable([1, 1, num_channels, 16])\n",
    "    b_conv3_1 = bias_variable([16])\n",
    "    A_conv3_1 = tf.nn.relu(tf.nn.conv2d(X, W_conv3_1, strides = [1, 1, 1, 1], \n",
    "                                        padding = 'VALID') + b_conv3_1)\n",
    "    \n",
    "    W_conv3_2 = weight_variable([5, 5, 16, 32])\n",
    "    b_conv3_2 = bias_variable([32])\n",
    "    A_conv3_2 = tf.nn.relu(tf.nn.conv2d(A_conv3_1, W_conv3_2, strides = [1, 1, 1, 1], \n",
    "                                        padding = 'SAME') + b_conv3_2)\n",
    "    \n",
    "    # Branch 4\n",
    "    A_pool4 = tf.nn.max_pool(X, ksize=[1, 3, 3, 1], strides = [1, 1, 1, 1], padding = 'SAME')\n",
    "    W_conv4_1 = weight_variable([1, 1, num_channels, 32])\n",
    "    b_conv4_1 = bias_variable([32])\n",
    "    A_conv4_1 = tf.nn.relu(tf.nn.conv2d(A_pool4, W_conv4_1, strides = [1, 1, 1, 1], \n",
    "                                        padding = 'VALID') + b_conv4_1)\n",
    "    \n",
    "    # Concatenate\n",
    "    A_conv5 = tf.concat([A_conv1_1, A_conv2_2, A_conv3_2, A_conv4_1], axis = -1)\n",
    "    \n",
    "    return A_conv5\n",
    "\n",
    "# Placeholders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# First convolutional layer (64 3x3x1 filters)\n",
    "X_image = tf.reshape(X, [-1, 28, 28, 1])\n",
    "W_conv1 = weight_variable([3, 3, 1, 64])\n",
    "b_conv1 = bias_variable([64])\n",
    "A_conv1 = tf.nn.relu(conv2d(X_image, W_conv1) + b_conv1) \n",
    "# Output shape (m, 26, 26, 64)\n",
    "\n",
    "# Second convolutional layer (128 3x3x64 filters > max pool)\n",
    "W_conv2 = weight_variable([3, 3, 64, 128])\n",
    "b_conv2 = bias_variable([128])\n",
    "A_conv2 = tf.nn.relu(conv2d(A_conv1, W_conv2) + b_conv2)\n",
    "A_pool2 = tf.nn.max_pool(A_conv2, ksize=[1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME') \n",
    "# Output shape (m, 12, 12, 128)\n",
    "\n",
    "# Inception block (x3)\n",
    "block1 = inception_block(A_pool2)\n",
    "block2 = inception_block(block1)\n",
    "block3 = inception_block(block2)\n",
    "block3_flat = tf.reshape(block3, [-1, 12*12*256])\n",
    "# Output shape (m, 36864)\n",
    "\n",
    "# Output softmax layer (10 units corresponding to the labels 0-9)\n",
    "W_fc9 = weight_variable([12*12*256, 10])\n",
    "b_fc9 = bias_variable([10])\n",
    "Z_fc9 = tf.matmul(block3_flat, W_fc9) + b_fc9 # Logit\n",
    "\n",
    "# Cost\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = Z_fc9))\n",
    "\n",
    "# Single training step\n",
    "train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)\n",
    "\n",
    "# Accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(Z_fc9, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Create a session\n",
    "sess = tf.InteractiveSession()\n",
    "# Initialize variables\n",
    "tf.global_variables_initializer().run()\n",
    "# Perform 10000 iterations of backward and forward propagation\n",
    "for i in range(1, 10001):\n",
    "    batch = dataset.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict = {X: batch[0], y: batch[1]})\n",
    "    if i % 500 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict = {X: batch[0], y: batch[1]})\n",
    "        print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "\n",
    "# Test accuracy\n",
    "test_acc = 0\n",
    "for i in range(100): # Separate the test set into 100 batches to avoid ResourceExhaustedError\n",
    "    test_batch = dataset.test.next_batch(100)\n",
    "    test_acc += accuracy.eval(feed_dict = {X: test_batch[0], y: test_batch[1]})\n",
    "test_acc /= 100\n",
    "print('test accuracy', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 500, training accuracy 0.95\n",
      "step 1000, training accuracy 0.99\n",
      "step 1500, training accuracy 1\n",
      "step 2000, training accuracy 0.99\n",
      "step 2500, training accuracy 1\n",
      "step 3000, training accuracy 1\n",
      "step 3500, training accuracy 1\n",
      "step 4000, training accuracy 1\n",
      "step 4500, training accuracy 1\n",
      "step 5000, training accuracy 1\n",
      "step 5500, training accuracy 1\n",
      "step 6000, training accuracy 1\n",
      "step 6500, training accuracy 1\n",
      "step 7000, training accuracy 1\n",
      "step 7500, training accuracy 1\n",
      "step 8000, training accuracy 1\n",
      "step 8500, training accuracy 1\n",
      "step 9000, training accuracy 1\n",
      "step 9500, training accuracy 1\n",
      "step 10000, training accuracy 1\n",
      "test accuracy 0.9918000072240829\n"
     ]
    }
   ],
   "source": [
    "# Residual Network\n",
    "\"\"\"\n",
    "Experiments have shown that very deep networks degrade. The intuition is that performance worsens \n",
    "because it is difficult for a deep network to learn an identity mapping between layers. Residual\n",
    "networks (ResNets) combined with the use of batch normalization helps resolve this problem. Using a \n",
    "ResNet, we are able to train networks with over 100 layers without a drop in performance. This is \n",
    "because ResNets are able to easily learn an identity mapping between consecutive layers. A ResNet \n",
    "is made up of a series of residual blocks. These blocks contain a 'skip connection' that allows \n",
    "the gradient to be directly backpropagated to earlier layers. There are 2 main types of residual \n",
    "blocks; the identity block (input has same dimension as output) and the convolutional block (input \n",
    "has a different dimension from output). A ResNet is made up of a series of many of these different \n",
    "blocks. This particular Resnet consists of only 65 layers, however, they are more commonly used for\n",
    "much deeper networks. Note that batch normalization acts as a regulator.\n",
    "\"\"\"\n",
    "\n",
    "# Parameter initialization functions\n",
    "def weight_variable(shape):\n",
    "    initializer = tf.contrib.layers.xavier_initializer()\n",
    "    return tf.Variable(initializer(shape))\n",
    "\n",
    "def bias_variable(shape):\n",
    "    return tf.Variable(tf.zeros(shape))\n",
    "\n",
    "# Define identity block function\n",
    "def identity_block(X, f, filters):\n",
    "    F1, F2, F3 = filters\n",
    "    W_conv1 = weight_variable([1, 1, F3, F1]) #F1 filters of shape (1,1,F3)\n",
    "    h_conv1 = tf.nn.relu(tf.layers.batch_normalization(\n",
    "            tf.nn.conv2d(X, W_conv1, strides = [1, 1, 1, 1], padding = 'VALID'), axis = 2))\n",
    "    W_conv2 = weight_variable([f, f, F1, F2]) #F2 filters of shape (f,f,F1)\n",
    "    h_conv2 = tf.nn.relu(tf.layers.batch_normalization(\n",
    "            tf.nn.conv2d(h_conv1, W_conv2, strides = [1, 1, 1, 1], padding = 'SAME'), axis = 2)) \n",
    "    W_conv3 = weight_variable([1, 1, F2, F3]) #F3 filters of shape (1,1,F2)\n",
    "    h_conv3 = tf.nn.relu(tf.layers.batch_normalization(\n",
    "            tf.nn.conv2d(h_conv2, W_conv3, strides = [1, 1, 1, 1], padding = 'VALID'), axis = 2) + X)\n",
    "            #F3 = number channels in X\n",
    "    return h_conv3\n",
    "\n",
    "# Define convolutional block function\n",
    "def convolutional_block(X, f, filters, s = 2):\n",
    "    F1, F2, F3 = filters\n",
    "    num_channels = X.get_shape()[3].value\n",
    "    \n",
    "    W_conv1 = weight_variable([1, 1, num_channels, F1]) #F1 filters of shape (1,1,num_channels)\n",
    "    h_conv1 = tf.nn.relu(tf.layers.batch_normalization(\n",
    "            tf.nn.conv2d(X, W_conv1, strides = [1, s, s, 1], padding = 'VALID'), axis = 2)) \n",
    "    W_conv2 = weight_variable([f, f, F1, F2]) #F2 filters of shape (f,f,F1)\n",
    "    h_conv2 = tf.nn.relu(tf.layers.batch_normalization(\n",
    "            tf.nn.conv2d(h_conv1, W_conv2, strides = [1, 1, 1, 1], padding = 'SAME'), axis = 2)) \n",
    "    W_conv3 = weight_variable([1, 1, F2, F3]) #F3 filters of shape (1,1,F2)\n",
    "    z_conv3 = tf.layers.batch_normalization(\n",
    "            tf.nn.conv2d(h_conv2, W_conv3, strides = [1, 1, 1, 1], padding = 'VALID'), axis = 2) \n",
    "    W_shortcut = weight_variable([1, 1, num_channels, F3]) #F3 filters of shape (1,1,num_channels)\n",
    "    z_shortcut = tf.layers.batch_normalization(\n",
    "            tf.nn.conv2d(X, W_shortcut, strides = [1, s, s, 1], padding = 'VALID'), axis = 2) \n",
    "    h_conv3 = tf.nn.relu(z_conv3 + z_shortcut) \n",
    "    \n",
    "    return h_conv3\n",
    "\n",
    "# Placeholders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# Zero padding\n",
    "X_image = tf.reshape(X, [-1, 28, 28, 1])\n",
    "zeropad = tf.pad(X_image, [[0,0], [2, 2], [2, 2],[0,0]]) \n",
    "# Output shape (m, 32, 32, 1)\n",
    "\n",
    "# First convolutional layer\n",
    "W_conv1 = weight_variable([3, 3, 1, 32])\n",
    "A_conv1 = tf.nn.relu(tf.layers.batch_normalization(\n",
    "    tf.nn.conv2d(zeropad, W_conv1, strides = [1, 2, 2, 1], padding = 'SAME'), axis = 2)) \n",
    "# Output shape (m, 16, 16, 32)\n",
    "\n",
    "# Residual block (x12)\n",
    "block1 = convolutional_block(A_conv1, f = 5, filters = [32, 32, 128], s = 1)\n",
    "block2 = identity_block(block1, f = 5, filters = [32, 32, 128])\n",
    "block3 = identity_block(block2, f = 5, filters = [32, 32, 128])\n",
    "\n",
    "block4 = convolutional_block(block3, f = 3, filters = [64, 64, 256])\n",
    "block5 = identity_block(block4, f = 3, filters = [64, 64, 256])\n",
    "block6 = identity_block(block5, f = 3, filters = [64, 64, 256])\n",
    "block7 = identity_block(block6, f = 3, filters = [64, 64, 256])\n",
    "\n",
    "block8 = convolutional_block(block7, f = 1, filters = [128, 128, 512])\n",
    "block9 = identity_block(block8, f = 1, filters = [128, 128, 512])\n",
    "block10 = identity_block(block9, f = 1, filters = [128, 128, 512])\n",
    "block11 = identity_block(block10, f = 1, filters = [128, 128, 512])\n",
    "block12 = identity_block(block11, f = 1, filters = [128, 128, 512])\n",
    "# Output shape (m, 4, 4, 512)\n",
    "\n",
    "#Output softmax layer (10 units corresponding to the labels 0-9)\n",
    "W_fc37 = weight_variable([4*4*512, 10])\n",
    "b_fc37 = bias_variable([10])\n",
    "block12_flat = tf.reshape(block12, [-1, 4*4*512])\n",
    "Z_fc37 = tf.matmul(block12_flat, W_fc37) + b_fc37 # Logit\n",
    "\n",
    "# Cost\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = Z_fc37))\n",
    "\n",
    "# Single training step\n",
    "train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)\n",
    "\n",
    "# Accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(Z_fc37, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Create a session\n",
    "sess = tf.InteractiveSession()\n",
    "# Initialize variables\n",
    "tf.global_variables_initializer().run()\n",
    "# Perform 10000 iterations of backward and forward propagation\n",
    "for i in range(1, 10001):\n",
    "    batch = dataset.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict = {X: batch[0], y: batch[1]})\n",
    "    if i % 500 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict = {X: batch[0], y: batch[1]})\n",
    "        print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "\n",
    "# Test accuracy\n",
    "test_acc = 0\n",
    "for i in range(100): # Separate the test set into 100 batches to avoid ResourceExhaustedError\n",
    "    test_batch = dataset.test.next_batch(100)\n",
    "    test_acc += accuracy.eval(feed_dict = {X: test_batch[0], y: test_batch[1]})\n",
    "test_acc /= 100\n",
    "print('test accuracy', test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
